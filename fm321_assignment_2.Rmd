---
title: "Assignment 2 - Candidate Number: 39756"
output: html_document
date: "2024-12-14"
---

## Question:
Risk Forecasting With Machine Learning (ML).
Select three price series, load them into R, and convert them to returns. 
Briefly describe the data statistically. 
Create a ML forecasting model for risk. This question must be executed in R.



**The code in this document was used for my assignment report, specifically for each of the model tests and plots**
Approximately 80% of this code was used for these plots and the other 20% was more exploratory analysis.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("ggplot2")
library("reshape2")
library("lubridate")
library("zoo")
library("ggplot2")
library("scales")
library("dplyr", warn.conflicts = FALSE)
library("tidyr")
library("gridExtra")
library("grid")
library("rugarch")
library("rpart")
library("rpart.plot")
library("caret")
library("randomForest")
library("xgboost")
library("tidyverse")
library("TTR")
library("keras")
library("pROC")
library("NeuralNetTools")
library("DiagrammeR")
library(reticulate)
use_python("/Users/michaeldebruijn/tensorflow_env/bin/python")

library("tensorflow")
```

## Data prep
From WRDS we have:
•	Price (PRC) which is unadjusted
•	CPAFR (price adjustment factor to change PRC into actual adjusted price to account for stock splits etc.)
•	Daily returns (RET)
•	Trading volume (vol)
•	S&P500 index returns (for market factor)


Commencing date is 1 day after Tesla IPO to be able to get returns and keep consistent

```{r, setting up data, include=FALSE}
data <- read.csv("crsp2.csv")
data <- data[!data$date %in% head(unique(data$date), 2), ]

#adjusting prices using the factor, to account for stock splits
data$Unadjusted_Prices <- data$PRC
data$Adjusted_Prices <- data$PRC / data$CFACPR
head(data)

# normalising volume as it handles risk-skew of trading volumes well
data$normalized_vol = log(data$VOL)

# keeps only most recent ticker symbols and makes each row be a data, and each column a different stock
Tickers= dcast(data, date ~ PERMNO, value.var = "TICKER")
Tickers=tail(Tickers,1)
Tickers


Prices = dcast(data, date ~ PERMNO, value.var = "Adjusted_Prices")
names(Prices) <- Tickers
names(Prices)[1]="date"
head(Prices)
dim(Prices)
tail(Prices)

# Creating the data frame for simple returns:
simpleReturns <- dcast(data, date ~ PERMNO, value.var = "RET")
names(simpleReturns) <- Tickers
names(simpleReturns)[1]="date"
head(simpleReturns)


# Cleaning data - remove "C" corrected value and convert to numeric
simpleReturns$AAPL <- as.numeric(gsub("C", "", simpleReturns$AAPL))
simpleReturns$JPM <- as.numeric(gsub("C", "", simpleReturns$JPM))
simpleReturns$TSLA <- as.numeric(gsub("C", "", simpleReturns$TSLA))

# Transforming simple returns into compound returns
Returns <- log(1 + simpleReturns[,2:dim(simpleReturns)[2]])
Returns$date <- simpleReturns$date
Returns =Returns[,c(dim(Returns)[2],1:(dim(Returns)[2]-1))]
head(Returns)

# Putting the data column into the actual Date class type
date.ts <- ymd(Returns$date)

# Unadjusted prices variable
UnAdjustedPrices = dcast(data, date ~ PERMNO, value.var = "PRC")
names(UnAdjustedPrices) <- Tickers
names(UnAdjustedPrices)[1]="date"
```

```{r, cleaning data, include=FALSE}
# duplicating data and removing unneeded columns
data2 <- data %>% 
  select(date, TICKER, Adjusted_Prices, RET, normalized_vol, sprtrn)

# Create separate dataframes for each stock
apple_maindata <- data2 %>%
  filter(TICKER == "AAPL") %>%
  select(date, TICKER, Adjusted_Prices, RET, normalized_vol, sprtrn)

jpm_maindata <- data2 %>%
  filter(TICKER == "JPM") %>%
  select(date, TICKER, Adjusted_Prices, RET, normalized_vol, sprtrn)

tesla_maindata <- data2 %>%
  filter(TICKER == "TSLA") %>%
  select(date, TICKER, Adjusted_Prices, RET, normalized_vol, sprtrn)

# Convert dates to proper format
apple_maindata$date <- ymd(apple_maindata$date)
jpm_maindata$date <- ymd(jpm_maindata$date)
tesla_maindata$date <- ymd(tesla_maindata$date)

# Check the number of rows in each dataframe
nrow(apple_maindata)
nrow(jpm_maindata)
nrow(tesla_maindata)

```



```{r, cleaning data, include=FALSE}
# duplicating data and removing unneeded columns
data2 <- data %>% 
  select(date, TICKER, Adjusted_Prices, RET, normalized_vol, sprtrn)
```


## ESSAY PLAN:
1.	Pick 3 stocks (blue chip: AAPL, finance: JPM, growth/volatile stock: TSLA)
2.	Clean and normalise data 
3.	Plot returns to visualise volatility clustering and do some simple statistical analysis
4.	Define success in this case, e.g. the lowest MSE / RMSE, but also directional accuracy (percentage of correct UPS/DOWNS predicted)
5.	Fit GARCH model using the 80% train data only. Then run it on 20% test data, plot it, and use as benchmark reference point. 
6.	Split data 80-20 train to test split
7.	Decision trees
8.	Random Forests (ensemble of trees)
9.	XGBoost - GBM model based on decision trees essentially
10.	Neural networks (if there is time)
11.	Compare the ML models against historical average benchmark and standard GARCH model
12.	Discuss which model is performing better and why and how can be improved (maybe include market index and VIX, volatility index)



**Plot returns**

```{r}
# Convert date column to proper date format
Returns$date <- ymd(Returns$date)

# Calculate compound returns as percentages (starting with 0%)
cumulative_returns <- Returns[, c("date", "AAPL", "JPM", "TSLA")]
cumulative_returns$AAPL <- (exp(cumsum(cumulative_returns$AAPL)) - 1) * 100
cumulative_returns$JPM <- (exp(cumsum(cumulative_returns$JPM)) - 1) * 100
cumulative_returns$TSLA <- (exp(cumsum(cumulative_returns$TSLA)) - 1) * 100

# Create individual plots
plot_jpm <- ggplot(cumulative_returns, aes(x = date, y = JPM)) +
  geom_line(color = "red") +
  theme_minimal() +
  labs(title = "JPMorgan Chase",
       x = "Date",
       y = "Cumulative Return (%)") +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  theme(plot.title = element_text(hjust = 0.5, size = 12, face = "bold"),
        axis.title = element_text(size = 10),
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_aapl <- ggplot(cumulative_returns, aes(x = date, y = AAPL)) +
  geom_line(color = "blue") +
  theme_minimal() +
  labs(title = "Apple",
       x = "Date",
       y = "Cumulative Return (%)") +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  theme(plot.title = element_text(hjust = 0.5, size = 12, face = "bold"),
        axis.title = element_text(size = 10),
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_tsla <- ggplot(cumulative_returns, aes(x = date, y = TSLA)) +
  geom_line(color = "green4") +
  theme_minimal() +
  labs(title = "Tesla",
       x = "Date",
       y = "Cumulative Return (%)") +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  theme(plot.title = element_text(hjust = 0.5, size = 12, face = "bold"),
        axis.title = element_text(size = 10),
        axis.text.x = element_text(angle = 45, hjust = 1))

# Arrange plots in a row
grid.arrange(plot_jpm, plot_aapl, plot_tsla, ncol = 3,
            top = textGrob("Cumulative Stock Returns",
                          gp = gpar(fontsize = 14, fontface = "bold")))
```


```{r, splitting data, include=FALSE}
# Calculate split point (80% of the data)
split_date_apple <- apple_maindata$date[floor(nrow(apple_maindata) * 0.8)]
split_date_jpm <- jpm_maindata$date[floor(nrow(jpm_maindata) * 0.8)]
split_date_tesla <- tesla_maindata$date[floor(nrow(tesla_maindata) * 0.8)]

# Split Apple data
apple_train <- apple_maindata[apple_maindata$date <= split_date_apple, ]
apple_test <- apple_maindata[apple_maindata$date > split_date_apple, ]

# Split JPM data
jpm_train <- jpm_maindata[jpm_maindata$date <= split_date_jpm, ]
jpm_test <- jpm_maindata[jpm_maindata$date > split_date_jpm, ]

# Split Tesla data
tesla_train <- tesla_maindata[tesla_maindata$date <= split_date_tesla, ]
tesla_test <- tesla_maindata[tesla_maindata$date > split_date_tesla, ]

# Print the dimensions of training and testing sets
cat("Apple Training Set:", nrow(apple_train), "observations\n")
cat("Apple Testing Set:", nrow(apple_test), "observations\n")
cat("Training period:", min(apple_train$date), "to", max(apple_train$date), "\n")
cat("Testing period:", min(apple_test$date), "to", max(apple_test$date), "\n\n")

cat("JPM Training Set:", nrow(jpm_train), "observations\n")
cat("JPM Testing Set:", nrow(jpm_test), "observations\n")
cat("Training period:", min(jpm_train$date), "to", max(jpm_train$date), "\n")
cat("Testing period:", min(jpm_test$date), "to", max(jpm_test$date), "\n\n")

cat("Tesla Training Set:", nrow(tesla_train), "observations\n")
cat("Tesla Testing Set:", nrow(tesla_test), "observations\n")
cat("Training period:", min(tesla_train$date), "to", max(tesla_train$date), "\n")
cat("Testing period:", min(tesla_test$date), "to", max(tesla_test$date), "\n")

# Optional: Verify that we have no missing values in our splits
cat("\nMissing values in training sets:\n")
cat("Apple:", sum(is.na(apple_train$RET)), "\n")
cat("JPM:", sum(is.na(jpm_train$RET)), "\n")
cat("Tesla:", sum(is.na(tesla_train$RET)), "\n")

cat("\nMissing values in testing sets:\n")
cat("Apple:", sum(is.na(apple_test$RET)), "\n")
cat("JPM:", sum(is.na(jpm_test$RET)), "\n")
cat("Tesla:", sum(is.na(tesla_test$RET)), "\n")
```




### GARCH Model for Value at Risk (not included in final paper)


For Apple first:

```{r}
# Ensure returns are numeric
apple_train$RET <- as.numeric(as.character(apple_train$RET))
apple_test$RET <- as.numeric(as.character(apple_test$RET))

# Check for any NAs after conversion
cat("\nNA values in training set:", sum(is.na(apple_train$RET)), "\n")
cat("NA values in test set:", sum(is.na(apple_test$RET)), "\n")

# Function to calculate GARCH VaR with error checking
calculate_garch_var <- function(returns, p = 0.95) {
    # Check if returns are numeric
    if(!is.numeric(returns)) {
        stop("Returns must be numeric")
    }
    
    # Remove any NAs
    returns <- na.omit(returns)
    
    # Specify GARCH model
    spec <- ugarchspec(
        variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
        mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
        distribution.model = "norm"
    )
    
    # Fit the model
    fit <- ugarchfit(spec = spec, data = returns, solver = "hybrid")
    
    # Extract conditional volatilities
    sigma <- sigma(fit)
    
    # Calculate VaR
    var <- -qnorm(p) * sigma
    
    return(var)
}

# Calculate VaR for training period
train_var <- calculate_garch_var(apple_train$RET)

# Calculate VaR for test period using expanding window approach
test_var <- numeric(length(apple_test$RET))
full_returns <- c(apple_train$RET, apple_test$RET)

# Print diagnostic information
cat("\nDimensions:\n")
cat("Length of test set:", length(apple_test$RET), "\n")
cat("Length of full returns:", length(full_returns), "\n")

for(i in 1:length(apple_test$RET)) {
    # Get data up to this point
    current_data <- full_returns[1:(length(apple_train$RET) + i)]
    # Calculate VaR using all available data
    var <- calculate_garch_var(current_data)
    # Store the last VaR value
    test_var[i] <- tail(var, 1)
}

# Create plot data with explicit type conversion
plot_data <- data.frame(
    date = apple_test$date,
    returns = as.numeric(apple_test$RET),
    var_95 = as.numeric(test_var)
)

# Check plot data structure
cat("\nStructure of plot_data:\n")
str(plot_data)

# Create plot
vol_plot <- ggplot(plot_data, aes(x = date)) +
    geom_line(aes(y = returns, color = "Returns")) +
    geom_line(aes(y = var_95, color = "95% VaR"), linetype = "dashed") +
    labs(
        title = "Apple Returns vs 95% VaR (Adaptive GARCH)",
        x = "Date",
        y = "Returns/VaR",
        color = "Series"
    ) +
    theme_minimal() +
    theme(
        legend.position = "bottom",
        plot.title = element_text(size = 12, face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)
    ) +
    scale_color_manual(values = c("Returns" = "black", "95% VaR" = "red"))

# Print the plot
print(vol_plot)

# Calculate VaR violations with type checking
violations <- sum(as.numeric(plot_data$returns) < as.numeric(plot_data$var_95), na.rm = TRUE)
violation_rate <- violations / sum(!is.na(plot_data$returns))

cat("\nVaR Backtesting Results:\n")
cat("Number of VaR violations:", violations, "\n")
cat("VaR violation rate:", round(violation_rate * 100, 2), "%\n")
cat("Expected violation rate: 5%\n")

# Calculate performance metrics with explicit type conversion
returns_numeric <- as.numeric(plot_data$returns)
var_numeric <- as.numeric(plot_data$var_95)

rmse <- sqrt(mean((abs(returns_numeric) - abs(var_numeric))^2, na.rm = TRUE))
mae <- mean(abs(abs(returns_numeric) - abs(var_numeric)), na.rm = TRUE)

cat("\nPerformance Metrics:\n")
cat("RMSE:", round(rmse, 4), "\n")
cat("MAE:", round(mae, 4), "\n")

# Print summary statistics
cat("\nSummary statistics:\n")
cat("Mean return:", mean(returns_numeric, na.rm = TRUE), "\n")
cat("Mean VaR:", mean(var_numeric, na.rm = TRUE), "\n")
cat("SD returns:", sd(returns_numeric, na.rm = TRUE), "\n")
cat("SD VaR:", sd(var_numeric, na.rm = TRUE), "\n")
```
Value at risk at 95% level means: says that there's a 95% chance your losses won't exceed this amount.
A violation happens when you lose MORE money than VaR predicted.
VaR predicted is our dashed red line, and the actual returns are the black line.
If our 95% VaR is working perfectly, violations should happen only 5% of the time. If there are too many violations then the model is underestimating risk, and if there are too few violations then the model is too conservative.




JPM:

***JPM GARCH RISK FORECASTING WITH RMSE AND MAE, AND DIRECTIONAL ACCURACY, USING 5-DAY WINDOWS***


```{r}
# Load required libraries
library(rugarch)
library(zoo)
library(pROC)

# First ensure returns are numeric
jpm_train$RET <- as.numeric(as.character(jpm_train$RET))
jpm_test$RET <- as.numeric(as.character(jpm_test$RET))

# Modify the calculate_garch_volatility function to return volatility predictions
calculate_garch_volatility <- function(returns) {
    if(!is.numeric(returns)) {
        stop("Returns must be numeric")
    }
    returns <- na.omit(returns)
    
    spec <- ugarchspec(
        variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
        mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
        distribution.model = "norm"
    )
    
    fit <- ugarchfit(spec = spec, data = returns, solver = "hybrid")
    
    # Extract conditional volatilities
    predicted_vol <- sigma(fit)
    return(predicted_vol)
}

# Calculate predicted volatilities for test period using expanding window
test_vol_predictions <- numeric(length(jpm_test$RET))
full_returns <- c(jpm_train$RET, jpm_test$RET)
for(i in 1:length(jpm_test$RET)) {
    current_data <- full_returns[1:(length(jpm_train$RET) + i)]
    vol <- calculate_garch_volatility(current_data)
    test_vol_predictions[i] <- tail(vol, 1)
}

# Now calculate 5-day measures with consistent methodology
# Calculate forward-looking (left-aligned) realized volatility
realized_vol_5d <- rollapply(jpm_test$RET, 
                           width = 5, 
                           FUN = function(x) sd(x) * sqrt(252), 
                           fill = NA, 
                           align = "left")

# Calculate current (right-aligned) volatility for comparison
current_vol_5d <- rollapply(jpm_test$RET,
                          width = 5,
                          FUN = function(x) sd(x) * sqrt(252),
                          fill = NA,
                          align = "right")

# Get 5-day predicted volatility from GARCH (annualized)
predicted_vol_5d <- rollmean(test_vol_predictions, k=5, fill=NA, align="right") * sqrt(252)

# Create binary outcomes for ROC analysis using consistent method
actual_direction <- as.factor(ifelse(realized_vol_5d > current_vol_5d, 1, 0))
predicted_direction <- as.factor(ifelse(predicted_vol_5d > current_vol_5d, 1, 0))

# Scale predictions to [0,1] for ROC curve
scaled_predictions <- (predicted_vol_5d - min(predicted_vol_5d, na.rm=TRUE)) / 
                     (max(predicted_vol_5d, na.rm=TRUE) - min(predicted_vol_5d, na.rm=TRUE))

# Calculate ROC and AUC (removing NA values)
valid_indices <- !is.na(actual_direction) & !is.na(scaled_predictions)
roc_obj <- roc(actual_direction[valid_indices], scaled_predictions[valid_indices])
auc_value <- auc(roc_obj)

# Save ROC object
roc_jpm_garch <- roc_obj

# Create confusion matrix
conf_matrix <- table(Predicted = predicted_direction, Actual = actual_direction)

# Calculate directional accuracy using 5-day windows
direction_correct_5d <- mean(predicted_direction == actual_direction, na.rm=TRUE) * 100

# Calculate metrics
rmse_vol_5d <- sqrt(mean((realized_vol_5d - predicted_vol_5d)^2, na.rm=TRUE))
mae_vol_5d <- mean(abs(realized_vol_5d - predicted_vol_5d), na.rm=TRUE)

# Print results
cat("\nVolatility Prediction Results (5-day windows):\n")
cat("RMSE for volatility:", round(rmse_vol_5d, 4), "\n")
cat("MAE for volatility:", round(mae_vol_5d, 4), "\n")
cat("Directional Accuracy (5-day):", round(direction_correct_5d, 2), "% \n")
cat("AUC:", round(auc_value, 3), "\n")
cat("\nMean predicted volatility:", round(mean(predicted_vol_5d, na.rm=TRUE), 4), "\n")
cat("Mean realized volatility:", round(mean(realized_vol_5d, na.rm=TRUE), 4), "\n")
cat("SD predicted volatility:", round(sd(predicted_vol_5d, na.rm=TRUE), 4), "\n")
cat("SD realized volatility:", round(sd(realized_vol_5d, na.rm=TRUE), 4), "\n")
cat("\nConfusion Matrix:\n")
print(conf_matrix)

# Plotting
par(mfrow=c(2,2))

# Plot 1: ROC Curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Plot 2: Actual vs Predicted Volatility
plot(realized_vol_5d, type="l", col="blue",
     main="Actual vs Predicted Volatility",
     xlab="Time", ylab="Volatility")
lines(predicted_vol_5d, col="red")
legend("topright", legend=c("Actual", "Predicted"),
       col=c("blue", "red"), lty=1)

# Plot 3: Volatility Scatter Plot
plot(realized_vol_5d, predicted_vol_5d,
     main="Predicted vs Actual Volatility",
     xlab="Actual Volatility",
     ylab="Predicted Volatility")
abline(0, 1, col="red", lty=2)

# Plot 4: Rolling Directional Accuracy
roll_acc <- rollmean(predicted_direction == actual_direction, k=20, fill=NA, align="right")
plot(roll_acc, type="l", col="blue",
     main="20-day Rolling Directional Accuracy",
     xlab="Time", ylab="Accuracy")



```


Tesla GARCH:
```{r}
# Load required libraries
library(rugarch)
library(zoo)
library(pROC)

# First ensure returns are numeric
tesla_train$RET <- as.numeric(as.character(tesla_train$RET))
tesla_test$RET <- as.numeric(as.character(tesla_test$RET))

# Modify the calculate_garch_volatility function to return volatility predictions
calculate_garch_volatility <- function(returns) {
    if(!is.numeric(returns)) {
        stop("Returns must be numeric")
    }
    returns <- na.omit(returns)
    
    spec <- ugarchspec(
        variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
        mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
        distribution.model = "norm"
    )
    
    fit <- ugarchfit(spec = spec, data = returns, solver = "hybrid")
    
    # Extract conditional volatilities
    predicted_vol <- sigma(fit)
    return(predicted_vol)
}

# Calculate predicted volatilities for test period using expanding window
test_vol_predictions <- numeric(length(tesla_test$RET))
full_returns <- c(tesla_train$RET, tesla_test$RET)

for(i in 1:length(tesla_test$RET)) {
    current_data <- full_returns[1:(length(tesla_train$RET) + i)]
    vol <- calculate_garch_volatility(current_data)
    test_vol_predictions[i] <- tail(vol, 1)
}

# Now calculate 5-day measures with consistent methodology
# Calculate forward-looking (left-aligned) realized volatility
realized_vol_5d <- rollapply(tesla_test$RET, 
                           width = 5, 
                           FUN = function(x) sd(x) * sqrt(252), 
                           fill = NA, 
                           align = "left")

# Calculate current (right-aligned) volatility for comparison
current_vol_5d <- rollapply(tesla_test$RET,
                          width = 5,
                          FUN = function(x) sd(x) * sqrt(252),
                          fill = NA,
                          align = "right")

# Get 5-day predicted volatility from GARCH (annualized)
predicted_vol_5d <- rollmean(test_vol_predictions, k=5, fill=NA, align="right") * sqrt(252)

# Create binary outcomes for ROC analysis using consistent method
actual_direction <- as.factor(ifelse(realized_vol_5d > current_vol_5d, 1, 0))
predicted_direction <- as.factor(ifelse(predicted_vol_5d > current_vol_5d, 1, 0))

# Scale predictions to [0,1] for ROC curve
scaled_predictions <- (predicted_vol_5d - min(predicted_vol_5d, na.rm=TRUE)) / 
                     (max(predicted_vol_5d, na.rm=TRUE) - min(predicted_vol_5d, na.rm=TRUE))

# Calculate ROC and AUC (removing NA values)
valid_indices <- !is.na(actual_direction) & !is.na(scaled_predictions)
roc_obj <- roc(actual_direction[valid_indices], scaled_predictions[valid_indices])
auc_value <- auc(roc_obj)

# Save ROC object
roc_tsla_garch <- roc_obj

# Create confusion matrix
conf_matrix <- table(Predicted = predicted_direction, Actual = actual_direction)

# Calculate directional accuracy using 5-day windows
direction_correct_5d <- mean(predicted_direction == actual_direction, na.rm=TRUE) * 100

# Calculate metrics
rmse_vol_5d <- sqrt(mean((realized_vol_5d - predicted_vol_5d)^2, na.rm=TRUE))
mae_vol_5d <- mean(abs(realized_vol_5d - predicted_vol_5d), na.rm=TRUE)

# Print results
cat("\nVolatility Prediction Results (5-day windows):\n")
cat("RMSE for volatility:", round(rmse_vol_5d, 4), "\n")
cat("MAE for volatility:", round(mae_vol_5d, 4), "\n")
cat("Directional Accuracy (5-day):", round(direction_correct_5d, 2), "% \n")
cat("AUC:", round(auc_value, 3), "\n")
cat("\nMean predicted volatility:", round(mean(predicted_vol_5d, na.rm=TRUE), 4), "\n")
cat("Mean realized volatility:", round(mean(realized_vol_5d, na.rm=TRUE), 4), "\n")
cat("SD predicted volatility:", round(sd(predicted_vol_5d, na.rm=TRUE), 4), "\n")
cat("SD realized volatility:", round(sd(realized_vol_5d, na.rm=TRUE), 4), "\n")

cat("\nConfusion Matrix:\n")
print(conf_matrix)

# Plotting
par(mfrow=c(2,2))

# Plot 1: ROC Curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Plot 2: Actual vs Predicted Volatility Time Series
plot(realized_vol_5d, type="l", col="orange", 
     main="5-Day Volatility: Actual vs Predicted",
     xlab="Time", ylab="Annualized Volatility")
lines(predicted_vol_5d, col="green")
legend("topright", legend=c("Actual", "Predicted"), 
       col=c("orange", "green"), lty=1)

# Plot 3: Predicted vs Actual Scatter Plot
plot(realized_vol_5d, predicted_vol_5d,
     main="Predicted vs Actual Volatility",
     xlab="Actual Volatility",
     ylab="Predicted Volatility")
abline(0, 1, col="red", lty=2)

# Plot 4: Rolling Directional Accuracy
roll_acc <- rollmean(predicted_direction == actual_direction, k=20, fill=NA, align="right")
plot(roll_acc, type="l", col="blue",
     main="20-day Rolling Directional Accuracy",
     xlab="Time", ylab="Accuracy")
```

Apple GARCH:
```{r}
# Load required libraries
library(rugarch)
library(zoo)
library(pROC)

# First ensure returns are numeric
apple_train$RET <- as.numeric(as.character(apple_train$RET))
apple_test$RET <- as.numeric(as.character(apple_test$RET))

# Modify the calculate_garch_volatility function to return volatility predictions
calculate_garch_volatility <- function(returns) {
    if(!is.numeric(returns)) {
        stop("Returns must be numeric")
    }
    returns <- na.omit(returns)
    
    spec <- ugarchspec(
        variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
        mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
        distribution.model = "norm"
    )
    
    fit <- ugarchfit(spec = spec, data = returns, solver = "hybrid")
    
    # Extract conditional volatilities
    predicted_vol <- sigma(fit)
    return(predicted_vol)
}

# Calculate predicted volatilities for test period using expanding window
test_vol_predictions <- numeric(length(apple_test$RET))
full_returns <- c(apple_train$RET, apple_test$RET)

for(i in 1:length(apple_test$RET)) {
    current_data <- full_returns[1:(length(apple_train$RET) + i)]
    vol <- calculate_garch_volatility(current_data)
    test_vol_predictions[i] <- tail(vol, 1)
}

# Now calculate 5-day measures with consistent methodology
# Calculate forward-looking (left-aligned) realized volatility
realized_vol_5d <- rollapply(apple_test$RET, 
                           width = 5, 
                           FUN = function(x) sd(x) * sqrt(252), 
                           fill = NA, 
                           align = "left")

# Calculate current (right-aligned) volatility for comparison
current_vol_5d <- rollapply(apple_test$RET,
                          width = 5,
                          FUN = function(x) sd(x) * sqrt(252),
                          fill = NA,
                          align = "right")

# Get 5-day predicted volatility from GARCH (annualized)
predicted_vol_5d <- rollmean(test_vol_predictions, k=5, fill=NA, align="right") * sqrt(252)

# Create binary outcomes for ROC analysis using consistent method
actual_direction <- as.factor(ifelse(realized_vol_5d > current_vol_5d, 1, 0))
predicted_direction <- as.factor(ifelse(predicted_vol_5d > current_vol_5d, 1, 0))

# Scale predictions to [0,1] for ROC curve
scaled_predictions <- (predicted_vol_5d - min(predicted_vol_5d, na.rm=TRUE)) / 
                     (max(predicted_vol_5d, na.rm=TRUE) - min(predicted_vol_5d, na.rm=TRUE))

# Calculate ROC and AUC (removing NA values)
valid_indices <- !is.na(actual_direction) & !is.na(scaled_predictions)
roc_obj <- roc(actual_direction[valid_indices], scaled_predictions[valid_indices])
auc_value <- auc(roc_obj)

# Save ROC object
roc_apple_garch <- roc_obj

# Create confusion matrix
conf_matrix <- table(Predicted = predicted_direction, Actual = actual_direction)

# Calculate directional accuracy using 5-day windows
direction_correct_5d <- mean(predicted_direction == actual_direction, na.rm=TRUE) * 100

# Calculate metrics
rmse_vol_5d <- sqrt(mean((realized_vol_5d - predicted_vol_5d)^2, na.rm=TRUE))
mae_vol_5d <- mean(abs(realized_vol_5d - predicted_vol_5d), na.rm=TRUE)

# Print results
cat("\nVolatility Prediction Results (5-day windows):\n")
cat("RMSE for volatility:", round(rmse_vol_5d, 4), "\n")
cat("MAE for volatility:", round(mae_vol_5d, 4), "\n")
cat("Directional Accuracy (5-day):", round(direction_correct_5d, 2), "% \n")
cat("AUC:", round(auc_value, 3), "\n")
cat("\nMean predicted volatility:", round(mean(predicted_vol_5d, na.rm=TRUE), 4), "\n")
cat("Mean realized volatility:", round(mean(realized_vol_5d, na.rm=TRUE), 4), "\n")
cat("SD predicted volatility:", round(sd(predicted_vol_5d, na.rm=TRUE), 4), "\n")
cat("SD realized volatility:", round(sd(realized_vol_5d, na.rm=TRUE), 4), "\n")

cat("\nConfusion Matrix:\n")
print(conf_matrix)

# Plotting
par(mfrow=c(2,2))

# Plot 1: ROC Curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Plot 2: Actual vs Predicted Volatility Time Series
plot(realized_vol_5d, type="l", col="orange", 
     main="5-Day Volatility: Actual vs Predicted",
     xlab="Time", ylab="Annualized Volatility")
lines(predicted_vol_5d, col="green")
legend("topright", legend=c("Actual", "Predicted"), 
       col=c("orange", "green"), lty=1)

# Plot 3: Predicted vs Actual Scatter Plot
plot(realized_vol_5d, predicted_vol_5d,
     main="Predicted vs Actual Volatility",
     xlab="Actual Volatility",
     ylab="Predicted Volatility")
abline(0, 1, col="red", lty=2)

# Plot 4: Rolling Directional Accuracy
roll_acc <- rollmean(predicted_direction == actual_direction, k=20, fill=NA, align="right")
plot(roll_acc, type="l", col="blue",
     main="20-day Rolling Directional Accuracy",
     xlab="Time", ylab="Accuracy")
```



### Example of how ONE DAY VOLATILITY PREDICTION IS NOT GREAT FOR GARCH, WITH ONLY 28% DIRECTIONAL ACCURACY FOR TESLA WHICH IS <50% AVG EXPECTED



```{r}
# Load required libraries
library(rugarch)
library(zoo)

# First ensure returns are numeric
tesla_train$RET <- as.numeric(as.character(tesla_train$RET))
tesla_test$RET <- as.numeric(as.character(tesla_test$RET))

# Modify the calculate_garch_volatility function to return volatility predictions
calculate_garch_volatility <- function(returns) {
    if(!is.numeric(returns)) {
        stop("Returns must be numeric")
    }
    returns <- na.omit(returns)
    
    spec <- ugarchspec(
        variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
        mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),
        distribution.model = "norm"
    )
    
    fit <- ugarchfit(spec = spec, data = returns, solver = "hybrid")
    
    # Extract conditional volatilities
    predicted_vol <- sigma(fit)
    return(predicted_vol)
}

# Calculate predicted volatilities for test period using expanding window
test_vol_predictions <- numeric(length(tesla_test$RET))
full_returns <- c(tesla_train$RET, tesla_test$RET)

for(i in 1:length(tesla_test$RET)) {
    current_data <- full_returns[1:(length(tesla_train$RET) + i)]
    vol <- calculate_garch_volatility(current_data)
    test_vol_predictions[i] <- tail(vol, 1)
}

# Annualize the predicted volatilities
test_vol_predictions_annualized <- test_vol_predictions * sqrt(252)

# Calculate realized next-day volatility using absolute returns
realized_vol <- abs(tesla_test$RET) * sqrt(252)  # Annualized absolute return

# Calculate current volatility for comparison
current_vol <- abs(lag(tesla_test$RET)) * sqrt(252)

# Create comparison dataframe
vol_comparison <- data.frame(
    date = tesla_test$date,
    predicted_vol = test_vol_predictions_annualized,
    realized_vol = realized_vol,
    current_vol = current_vol
)

# Calculate accuracy metrics
rmse_vol <- sqrt(mean((realized_vol - test_vol_predictions_annualized)^2, na.rm = TRUE))
mae_vol <- mean(abs(realized_vol - test_vol_predictions_annualized), na.rm = TRUE)

# Calculate directional accuracy
predicted_direction <- diff(test_vol_predictions_annualized) > 0
actual_direction <- diff(realized_vol) > 0
direction_correct <- mean(predicted_direction == actual_direction, na.rm = TRUE) * 100

# Print improved results
cat("\nVolatility Prediction Results (Annualized):\n")
cat("RMSE for volatility:", round(rmse_vol, 4), "\n")
cat("MAE for volatility:", round(mae_vol, 4), "\n")
cat("Directional Accuracy:", round(direction_correct, 2), "% \n")
cat("\nMean predicted volatility:", 
    round(mean(test_vol_predictions_annualized, na.rm = TRUE), 4), "\n")
cat("Mean realized volatility:", 
    round(mean(realized_vol, na.rm = TRUE), 4), "\n")
cat("SD predicted volatility:", 
    round(sd(test_vol_predictions_annualized, na.rm = TRUE), 4), "\n")
cat("SD realized volatility:", 
    round(sd(realized_vol, na.rm = TRUE), 4), "\n")


```



## Decision Tree

Decision tree struggled with predicting volatility, and is more suited for binary classification, so decided to focus on that to train for directional accuracy. Which performed better. Aim is to expand on this with random forests and XGboost to also be able to predict volatility with a good RMSE/MAE and also a good directional accuracy.

Using the same feature engineering function as used in the RF model to be consistent across all models

*JPM:*
```{r}
# Load required libraries
library(rpart)
library(rpart.plot)
library(dplyr)
library(zoo)
library(ggplot2)
library(gridExtra)
library(pROC)

# Set seed for reproducibility
set.seed(201)
RNGkind(sample.kind = "Rounding")

# Create copies of training and test data
dt_jpm_train <- jpm_train
dt_jpm_test <- jpm_test

# Using RF's feature engineering function for consistency
create_rf_features <- function(data) {
    data %>%
        arrange(date) %>%
        mutate(
            # Base return features
            ret_lag1 = lag(RET, 1),
            ret_lag2 = lag(RET, 2),
            ret_lag3 = lag(RET, 3),
            
            # Volatility features
            roll_vol_5d = rollapply(RET, width=5, FUN=sd, fill=NA, align="right"),
            roll_vol_10d = rollapply(RET, width=10, FUN=sd, fill=NA, align="right"),
            roll_vol_22d = rollapply(RET, width=22, FUN=sd, fill=NA, align="right"),
            
            # Volatility momentum
            vol_momentum_5d = roll_vol_5d - lag(roll_vol_5d, 5),
            vol_momentum_ratio = vol_momentum_5d / roll_vol_5d,
            
            # Return patterns
            abs_ret = abs(RET),
            avg_abs_ret_5d = rollapply(abs(RET), width=5, FUN=mean, fill=NA, align="right"),
            
            # Market features
            mkt_vol_5d = rollapply(sprtrn, width=5, FUN=sd, fill=NA, align="right"),
            mkt_vol_10d = rollapply(sprtrn, width=10, FUN=sd, fill=NA, align="right"),
            
            # Additional features
            market_rel_vol = roll_vol_5d / mkt_vol_5d,
            vol_trend_3d = rollapply(roll_vol_5d, 3, 
                                   function(x) (x[3] - x[1])/x[1], 
                                   fill=NA, align="right"),
            vol_ratio_5_22 = roll_vol_5d / roll_vol_22d,
            range_5d = rollapply(RET, width=5, 
                               FUN=function(x) max(x) - min(x), 
                               fill=NA, align="right"),
            
            # Feature interactions
            vol_range_ratio = range_5d / roll_vol_5d,
            momentum_range_interact = vol_momentum_5d * range_5d,
            vol_market_interact = roll_vol_5d * mkt_vol_5d,
            range_momentum_ratio = range_5d / (vol_momentum_5d + 1e-8)
        )
}

# Create targets (5-day forward volatility)
create_targets <- function(data) {
    df <- data.frame(data)
    # Calculate forward 5-day volatility
    df$next_vol <- rollapply(df$RET, 
                            width = 5, 
                            FUN = function(x) sd(x) * sqrt(252), 
                            fill = NA, 
                            align = "left")
    
    # Create binary target based on whether forward volatility increases
    df$current_vol <- rollapply(df$RET, 
                               width = 5, 
                               FUN = function(x) sd(x) * sqrt(252), 
                               fill = NA, 
                               align = "right")
    
    df$vol_direction <- factor(ifelse(df$next_vol > df$current_vol, 1, 0))
    
    return(df)
}

# Prepare data
dt_jpm_train <- create_rf_features(dt_jpm_train)
dt_jpm_train <- create_targets(dt_jpm_train)
dt_jpm_test <- create_rf_features(dt_jpm_test)
dt_jpm_test <- create_targets(dt_jpm_test)

# Remove NA values
dt_jpm_train <- na.omit(dt_jpm_train)
dt_jpm_test <- na.omit(dt_jpm_test)

# Define features (same as RF)
features <- c(
    "ret_lag1", "ret_lag2", "ret_lag3",
    "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
    "vol_momentum_5d", "vol_momentum_ratio",
    "abs_ret", "avg_abs_ret_5d",
    "mkt_vol_5d", "mkt_vol_10d",
    "market_rel_vol", "vol_trend_3d",
    "vol_ratio_5_22", "range_5d",
    "vol_range_ratio", "momentum_range_interact",
    "vol_market_interact", "range_momentum_ratio"
)

# Train models for both regression and classification
dt_model_reg <- rpart(next_vol ~ ., 
                     data = dt_jpm_train[, c(features, "next_vol")],
                     method = "anova",
                     control = rpart.control(maxdepth = 5,
                                          minsplit = 20,
                                          minbucket = 10,
                                          cp = 0.01))

dt_model_class <- rpart(vol_direction ~ ., 
                       data = dt_jpm_train[, c(features, "vol_direction")],
                       method = "class",
                       control = rpart.control(maxdepth = 5,
                                            minsplit = 20,
                                            minbucket = 10,
                                            cp = 0.01))

# Make predictions
train_pred_reg <- predict(dt_model_reg, dt_jpm_train)
test_pred_reg <- predict(dt_model_reg, dt_jpm_test)

train_pred_class <- predict(dt_model_class, dt_jpm_train, type = "class")
test_pred_class <- predict(dt_model_class, dt_jpm_test, type = "class")
test_pred_prob <- predict(dt_model_class, dt_jpm_test, type = "prob")

# Calculate metrics
# Regression metrics
train_rmse <- sqrt(mean((dt_jpm_train$next_vol - train_pred_reg)^2))
test_rmse <- sqrt(mean((dt_jpm_test$next_vol - test_pred_reg)^2))
train_mae <- mean(abs(dt_jpm_train$next_vol - train_pred_reg))
test_mae <- mean(abs(dt_jpm_test$next_vol - test_pred_reg))

# Calculate regression directional accuracy based on 5-day forward volatility
reg_direction <- ifelse(test_pred_reg > dt_jpm_test$current_vol, 1, 0)
reg_actual_direction <- ifelse(dt_jpm_test$next_vol > dt_jpm_test$current_vol, 1, 0)
direction_accuracy_reg <- mean(reg_direction == reg_actual_direction, na.rm = TRUE)

# Classification metrics
train_conf <- table(Predicted = train_pred_class, 
                   Actual = dt_jpm_train$vol_direction)
test_conf <- table(Predicted = test_pred_class, 
                  Actual = dt_jpm_test$vol_direction)

train_acc <- sum(diag(train_conf)) / sum(train_conf)
test_acc <- sum(diag(test_conf)) / sum(test_conf)

# Calculate ROC and AUC
roc_obj <- roc(dt_jpm_test$vol_direction, test_pred_prob[,2])
auc_value <- auc(roc_obj)

roc_jpm_dt <- roc_obj

# Print results
cat("\nRegression Results:\n")
cat("Training RMSE:", round(train_rmse, 6), "\n")
cat("Testing RMSE:", round(test_rmse, 6), "\n")
cat("Training MAE:", round(train_mae, 6), "\n")
cat("Testing MAE:", round(test_mae, 6), "\n")
cat("Directional Accuracy (Regression):", 
    round(direction_accuracy_reg * 100, 2), "%\n")

cat("\nClassification Results:\n")
cat("Training Accuracy:", round(train_acc * 100, 2), "%\n")
cat("Testing Accuracy:", round(test_acc * 100, 2), "%\n")
cat("AUC:", round(auc_value, 3), "\n")

# Print confusion matrices
cat("\nTest Set Confusion Matrix:\n")
print(test_conf)

# Plotting
par(mfrow=c(2,2))

# Plot decision tree for classification
rpart.plot(dt_model_class, 
          main="Classification Tree for Volatility Direction",
          extra=104,
          box.palette="GnBu")

# Plot ROC curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Plot actual vs predicted volatility
plot(dt_jpm_test$next_vol, type="l", col="blue",
     main="Actual vs Predicted Volatility (Test Set)",
     xlab="Time", ylab="Volatility")
lines(test_pred_reg, col="red")
legend("topright", legend=c("Actual", "Predicted"),
       col=c("blue", "red"), lty=1)

# Variable importance plot for regression
importance_scores <- dt_model_reg$variable.importance
if(length(importance_scores) > 0) {
    barplot(sort(importance_scores, decreasing = TRUE),
            main="Variable Importance (Regression)",
            las=2,
            col="darkred")
}
```
***APPLE DT***

```{r}
# Load required libraries
library(rpart)
library(rpart.plot)
library(dplyr)
library(zoo)
library(ggplot2)
library(gridExtra)
library(pROC)

# Set seed for reproducibility
set.seed(201)
RNGkind(sample.kind = "Rounding")

# Create copies of training and test data
dt_apple_train <- apple_train
dt_apple_test <- apple_test

# Use RF feature engineering function for consistency
create_rf_features <- function(data) {
    data %>%
        arrange(date) %>%
        mutate(
            # Base return features
            ret_lag1 = lag(RET, 1),
            ret_lag2 = lag(RET, 2),
            ret_lag3 = lag(RET, 3),
            
            # Volatility features
            roll_vol_5d = rollapply(RET, width=5, FUN=sd, fill=NA, align="right"),
            roll_vol_10d = rollapply(RET, width=10, FUN=sd, fill=NA, align="right"),
            roll_vol_22d = rollapply(RET, width=22, FUN=sd, fill=NA, align="right"),
            
            # Volatility momentum
            vol_momentum_5d = roll_vol_5d - lag(roll_vol_5d, 5),
            vol_momentum_ratio = vol_momentum_5d / roll_vol_5d,
            
            # Return patterns
            abs_ret = abs(RET),
            avg_abs_ret_5d = rollapply(abs(RET), width=5, FUN=mean, fill=NA, align="right"),
            
            # Market features
            mkt_vol_5d = rollapply(sprtrn, width=5, FUN=sd, fill=NA, align="right"),
            mkt_vol_10d = rollapply(sprtrn, width=10, FUN=sd, fill=NA, align="right"),
            
            # Additional features
            market_rel_vol = roll_vol_5d / mkt_vol_5d,
            vol_trend_3d = rollapply(roll_vol_5d, 3, 
                                   function(x) (x[3] - x[1])/x[1], 
                                   fill=NA, align="right"),
            vol_ratio_5_22 = roll_vol_5d / roll_vol_22d,
            range_5d = rollapply(RET, width=5, 
                               FUN=function(x) max(x) - min(x), 
                               fill=NA, align="right"),
            
            # Feature interactions
            vol_range_ratio = range_5d / roll_vol_5d,
            momentum_range_interact = vol_momentum_5d * range_5d,
            vol_market_interact = roll_vol_5d * mkt_vol_5d,
            range_momentum_ratio = range_5d / (vol_momentum_5d + 1e-8)
        )
}

# Create targets (5-day forward volatility)
create_targets <- function(data) {
    df <- data.frame(data)
    # Calculate forward 5-day volatility
    df$next_vol <- rollapply(df$RET, 
                            width = 5, 
                            FUN = function(x) sd(x) * sqrt(252), 
                            fill = NA, 
                            align = "left")
    
    # Create binary target based on whether forward volatility increases
    df$current_vol <- rollapply(df$RET, 
                               width = 5, 
                               FUN = function(x) sd(x) * sqrt(252), 
                               fill = NA, 
                               align = "right")
    
    df$vol_direction <- factor(ifelse(df$next_vol > df$current_vol, 1, 0))
    
    return(df)
}

# Prepare data
dt_apple_train <- create_rf_features(dt_apple_train)
dt_apple_train <- create_targets(dt_apple_train)
dt_apple_test <- create_rf_features(dt_apple_test)
dt_apple_test <- create_targets(dt_apple_test)

# Remove NA values
dt_apple_train <- na.omit(dt_apple_train)
dt_apple_test <- na.omit(dt_apple_test)

# Define features (same as RF)
features <- c(
    "ret_lag1", "ret_lag2", "ret_lag3",
    "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
    "vol_momentum_5d", "vol_momentum_ratio",
    "abs_ret", "avg_abs_ret_5d",
    "mkt_vol_5d", "mkt_vol_10d",
    "market_rel_vol", "vol_trend_3d",
    "vol_ratio_5_22", "range_5d",
    "vol_range_ratio", "momentum_range_interact",
    "vol_market_interact", "range_momentum_ratio"
)

# Train models for both regression and classification
dt_model_reg <- rpart(next_vol ~ ., 
                     data = dt_apple_train[, c(features, "next_vol")],
                     method = "anova",
                     control = rpart.control(maxdepth = 5,
                                          minsplit = 20,
                                          minbucket = 10,
                                          cp = 0.01))

dt_model_class <- rpart(vol_direction ~ ., 
                       data = dt_apple_train[, c(features, "vol_direction")],
                       method = "class",
                       control = rpart.control(maxdepth = 5,
                                            minsplit = 20,
                                            minbucket = 10,
                                            cp = 0.01))

# Make predictions
train_pred_reg <- predict(dt_model_reg, dt_apple_train)
test_pred_reg <- predict(dt_model_reg, dt_apple_test)

train_pred_class <- predict(dt_model_class, dt_apple_train, type = "class")
test_pred_class <- predict(dt_model_class, dt_apple_test, type = "class")
test_pred_prob <- predict(dt_model_class, dt_apple_test, type = "prob")

# Calculate metrics
# Regression metrics
train_rmse <- sqrt(mean((dt_apple_train$next_vol - train_pred_reg)^2))
test_rmse <- sqrt(mean((dt_apple_test$next_vol - test_pred_reg)^2))
train_mae <- mean(abs(dt_apple_train$next_vol - train_pred_reg))
test_mae <- mean(abs(dt_apple_test$next_vol - test_pred_reg))

# Calculate regression directional accuracy based on 5-day forward volatility
reg_direction <- ifelse(test_pred_reg > dt_apple_test$current_vol, 1, 0)
reg_actual_direction <- ifelse(dt_apple_test$next_vol > dt_apple_test$current_vol, 1, 0)
direction_accuracy_reg <- mean(reg_direction == reg_actual_direction, na.rm = TRUE)

# Classification metrics
train_conf <- table(Predicted = train_pred_class, 
                   Actual = dt_apple_train$vol_direction)
test_conf <- table(Predicted = test_pred_class, 
                  Actual = dt_apple_test$vol_direction)

train_acc <- sum(diag(train_conf)) / sum(train_conf)
test_acc <- sum(diag(test_conf)) / sum(test_conf)

# Calculate ROC and AUC
roc_obj <- roc(dt_apple_test$vol_direction, test_pred_prob[,2])
auc_value <- auc(roc_obj)

# Save the ROC object for later comparison
roc_apple_dt <- roc_obj

# Print results
cat("\nRegression Results:\n")
cat("Training RMSE:", round(train_rmse, 6), "\n")
cat("Testing RMSE:", round(test_rmse, 6), "\n")
cat("Training MAE:", round(train_mae, 6), "\n")
cat("Testing MAE:", round(test_mae, 6), "\n")
cat("Directional Accuracy (Regression):", 
    round(direction_accuracy_reg * 100, 2), "%\n")

cat("\nClassification Results:\n")
cat("Training Accuracy:", round(train_acc * 100, 2), "%\n")
cat("Testing Accuracy:", round(test_acc * 100, 2), "%\n")
cat("AUC:", round(auc_value, 3), "\n")

# Print confusion matrices
cat("\nTest Set Confusion Matrix:\n")
print(test_conf)

# Plotting
par(mfrow=c(2,2))

# Plot decision tree for classification
rpart.plot(dt_model_class, 
          main="Classification Tree for Volatility Direction",
          extra=104,
          box.palette="GnBu")

# Plot ROC curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Plot actual vs predicted volatility
plot(dt_apple_test$next_vol, type="l", col="blue",
     main="Actual vs Predicted Volatility (Test Set)",
     xlab="Time", ylab="Volatility")
lines(test_pred_reg, col="red")
legend("topright", legend=c("Actual", "Predicted"),
       col=c("blue", "red"), lty=1)

# Variable importance plot for regression
importance_scores <- dt_model_reg$variable.importance
if(length(importance_scores) > 0) {
    barplot(sort(importance_scores, decreasing = TRUE),
            main="Variable Importance (Regression)",
            las=2,
            col="darkred")
}
```
***DT FOR TESLA***

```{r}
# Load required libraries
library(rpart)
library(rpart.plot)
library(dplyr)
library(zoo)
library(ggplot2)
library(gridExtra)
library(pROC)

# Set seed for reproducibility
set.seed(201)
RNGkind(sample.kind = "Rounding")

# Create copies of training and test data
dt_tesla_train <- tesla_train
dt_tesla_test <- tesla_test

# Use RF feature engineering function for consistency
create_rf_features <- function(data) {
    data %>%
        arrange(date) %>%
        mutate(
            # Base return features
            ret_lag1 = lag(RET, 1),
            ret_lag2 = lag(RET, 2),
            ret_lag3 = lag(RET, 3),
            
            # Volatility features
            roll_vol_5d = rollapply(RET, width=5, FUN=sd, fill=NA, align="right"),
            roll_vol_10d = rollapply(RET, width=10, FUN=sd, fill=NA, align="right"),
            roll_vol_22d = rollapply(RET, width=22, FUN=sd, fill=NA, align="right"),
            
            # Volatility momentum
            vol_momentum_5d = roll_vol_5d - lag(roll_vol_5d, 5),
            vol_momentum_ratio = vol_momentum_5d / roll_vol_5d,
            
            # Return patterns
            abs_ret = abs(RET),
            avg_abs_ret_5d = rollapply(abs(RET), width=5, FUN=mean, fill=NA, align="right"),
            
            # Market features
            mkt_vol_5d = rollapply(sprtrn, width=5, FUN=sd, fill=NA, align="right"),
            mkt_vol_10d = rollapply(sprtrn, width=10, FUN=sd, fill=NA, align="right"),
            
            # Additional features
            market_rel_vol = roll_vol_5d / mkt_vol_5d,
            vol_trend_3d = rollapply(roll_vol_5d, 3, 
                                   function(x) (x[3] - x[1])/x[1], 
                                   fill=NA, align="right"),
            vol_ratio_5_22 = roll_vol_5d / roll_vol_22d,
            range_5d = rollapply(RET, width=5, 
                               FUN=function(x) max(x) - min(x), 
                               fill=NA, align="right"),
            
            # Feature interactions
            vol_range_ratio = range_5d / roll_vol_5d,
            momentum_range_interact = vol_momentum_5d * range_5d,
            vol_market_interact = roll_vol_5d * mkt_vol_5d,
            range_momentum_ratio = range_5d / (vol_momentum_5d + 1e-8)
        )
}

# Create targets (5-day forward volatility)
create_targets <- function(data) {
    df <- data.frame(data)
    # Calculate forward 5-day volatility
    df$next_vol <- rollapply(df$RET, 
                            width = 5, 
                            FUN = function(x) sd(x) * sqrt(252), 
                            fill = NA, 
                            align = "left")
    
    # Create binary target based on whether forward volatility increases
    df$current_vol <- rollapply(df$RET, 
                               width = 5, 
                               FUN = function(x) sd(x) * sqrt(252), 
                               fill = NA, 
                               align = "right")
    
    df$vol_direction <- factor(ifelse(df$next_vol > df$current_vol, 1, 0))
    
    return(df)
}

# Prepare data
dt_tesla_train <- create_rf_features(dt_tesla_train)
dt_tesla_train <- create_targets(dt_tesla_train)
dt_tesla_test <- create_rf_features(dt_tesla_test)
dt_tesla_test <- create_targets(dt_tesla_test)

# Remove NA values
dt_tesla_train <- na.omit(dt_tesla_train)
dt_tesla_test <- na.omit(dt_tesla_test)

# Define features (same as RF)
features <- c(
    "ret_lag1", "ret_lag2", "ret_lag3",
    "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
    "vol_momentum_5d", "vol_momentum_ratio",
    "abs_ret", "avg_abs_ret_5d",
    "mkt_vol_5d", "mkt_vol_10d",
    "market_rel_vol", "vol_trend_3d",
    "vol_ratio_5_22", "range_5d",
    "vol_range_ratio", "momentum_range_interact",
    "vol_market_interact", "range_momentum_ratio"
)

# Train models for both regression and classification
dt_model_reg <- rpart(next_vol ~ ., 
                     data = dt_tesla_train[, c(features, "next_vol")],
                     method = "anova",
                     control = rpart.control(maxdepth = 5,
                                          minsplit = 20,
                                          minbucket = 10,
                                          cp = 0.01))

dt_model_class <- rpart(vol_direction ~ ., 
                       data = dt_tesla_train[, c(features, "vol_direction")],
                       method = "class",
                       control = rpart.control(maxdepth = 5,
                                            minsplit = 20,
                                            minbucket = 10,
                                            cp = 0.01))

# Make predictions
train_pred_reg <- predict(dt_model_reg, dt_tesla_train)
test_pred_reg <- predict(dt_model_reg, dt_tesla_test)

train_pred_class <- predict(dt_model_class, dt_tesla_train, type = "class")
test_pred_class <- predict(dt_model_class, dt_tesla_test, type = "class")
test_pred_prob <- predict(dt_model_class, dt_tesla_test, type = "prob")

# Calculate metrics
# Regression metrics
train_rmse <- sqrt(mean((dt_tesla_train$next_vol - train_pred_reg)^2))
test_rmse <- sqrt(mean((dt_tesla_test$next_vol - test_pred_reg)^2))
train_mae <- mean(abs(dt_tesla_train$next_vol - train_pred_reg))
test_mae <- mean(abs(dt_tesla_test$next_vol - test_pred_reg))

# Calculate regression directional accuracy based on 5-day forward volatility
reg_direction <- ifelse(test_pred_reg > dt_tesla_test$current_vol, 1, 0)
reg_actual_direction <- ifelse(dt_tesla_test$next_vol > dt_tesla_test$current_vol, 1, 0)
direction_accuracy_reg <- mean(reg_direction == reg_actual_direction, na.rm = TRUE)

# Classification metrics
train_conf <- table(Predicted = train_pred_class, 
                   Actual = dt_tesla_train$vol_direction)
test_conf <- table(Predicted = test_pred_class, 
                  Actual = dt_tesla_test$vol_direction)

train_acc <- sum(diag(train_conf)) / sum(train_conf)
test_acc <- sum(diag(test_conf)) / sum(test_conf)

# Calculate ROC and AUC
roc_obj <- roc(dt_tesla_test$vol_direction, test_pred_prob[,2])
auc_value <- auc(roc_obj)

# Save the ROC object for later comparison
roc_tesla_dt <- roc_obj

# Print results
cat("\nRegression Results:\n")
cat("Training RMSE:", round(train_rmse, 6), "\n")
cat("Testing RMSE:", round(test_rmse, 6), "\n")
cat("Training MAE:", round(train_mae, 6), "\n")
cat("Testing MAE:", round(test_mae, 6), "\n")
cat("Directional Accuracy (Regression):", 
    round(direction_accuracy_reg * 100, 2), "%\n")

cat("\nClassification Results:\n")
cat("Training Accuracy:", round(train_acc * 100, 2), "%\n")
cat("Testing Accuracy:", round(test_acc * 100, 2), "%\n")
cat("AUC:", round(auc_value, 3), "\n")

# Print confusion matrices
cat("\nTest Set Confusion Matrix:\n")
print(test_conf)

# Plotting
par(mfrow=c(2,2))

# Plot decision tree for classification
rpart.plot(dt_model_class, 
          main="Classification Tree for Volatility Direction",
          extra=104,
          box.palette="GnBu")

# Plot ROC curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Plot actual vs predicted volatility
plot(dt_tesla_test$next_vol, type="l", col="blue",
     main="Actual vs Predicted Volatility (Test Set)",
     xlab="Time", ylab="Volatility")
lines(test_pred_reg, col="red")
legend("topright", legend=c("Actual", "Predicted"),
       col=c("blue", "red"), lty=1)

# Variable importance plot for regression
importance_scores <- dt_model_reg$variable.importance
if(length(importance_scores) > 0) {
    barplot(sort(importance_scores, decreasing = TRUE),
            main="Variable Importance (Regression)",
            las=2,
            col="darkred")
}
```



## Random Forest

```{r}
library(pROC)
set.seed(201)  # For reproducibility
RNGkind(sample.kind = "Rounding") 

# Create copies of training and test data
rf_jpm_train <- jpm_train
rf_jpm_test <- jpm_test

# Enhanced Feature Engineering Function with interactions (is consistent across models)
create_rf_features <- function(data) {
    df <- data.frame(data)
    
    # Base features
    df$ret_lag1 <- lag(df$RET, 1)
    df$ret_lag2 <- lag(df$RET, 2)
    df$ret_lag3 <- lag(df$RET, 3)
    
    # Volatility features
    df$roll_vol_5d <- rollapply(df$RET, width=5, FUN=sd, fill=NA, align="right") * sqrt(252)
    df$roll_vol_10d <- rollapply(df$RET, width=10, FUN=sd, fill=NA, align="right") * sqrt(252)
    df$roll_vol_22d <- rollapply(df$RET, width=22, FUN=sd, fill=NA, align="right") * sqrt(252)
    
    # Volatility momentum
    df$vol_momentum_5d <- df$roll_vol_5d - lag(df$roll_vol_5d, 5)
    df$vol_momentum_ratio <- df$vol_momentum_5d / df$roll_vol_5d
    
    # Return patterns
    df$abs_ret <- abs(df$RET)
    df$avg_abs_ret_5d <- rollapply(abs(df$RET), width=5, FUN=mean, fill=NA, align="right")
    
    # Market features
    df$mkt_vol_5d <- rollapply(df$sprtrn, width=5, FUN=sd, fill=NA, align="right") * sqrt(252)
    df$mkt_vol_10d <- rollapply(df$sprtrn, width=10, FUN=sd, fill=NA, align="right") * sqrt(252)
    
    # Additional features
    df$market_rel_vol <- df$roll_vol_5d / df$mkt_vol_5d
    df$vol_trend_3d <- rollapply(df$roll_vol_5d, 3, 
                                function(x) (x[3] - x[1])/x[1], 
                                fill=NA, align="right")
    df$vol_ratio_5_22 <- df$roll_vol_5d / df$roll_vol_22d
    df$range_5d <- rollapply(df$RET, width=5, 
                            FUN=function(x) max(x) - min(x), 
                            fill=NA, align="right")
    
    # Feature interactions
    df$vol_range_ratio <- df$range_5d / df$roll_vol_5d
    df$momentum_range_interact <- df$vol_momentum_5d * df$range_5d
    df$vol_market_interact <- df$roll_vol_5d * df$mkt_vol_5d
    df$range_momentum_ratio <- df$range_5d / (df$vol_momentum_5d + 1e-6)
    
    return(df)
}

# Create targets (now with left alignment and annualization)
create_targets <- function(data) {
    df <- data.frame(data)
    # Calculate forward-looking 5-day volatility (annualized)
    current_vol <- rollapply(df$RET, width=5, FUN=sd, fill=NA, align="right") * sqrt(252)
    df$next_vol <- rollapply(df$RET, width=5, FUN=sd, fill=NA, align="left") * sqrt(252)
    df$vol_direction <- factor(ifelse(df$next_vol > current_vol, 1, 0))
    return(df)
}

# Prepare data
rf_jpm_train <- create_rf_features(rf_jpm_train)
rf_jpm_train <- create_targets(rf_jpm_train)
rf_jpm_test <- create_rf_features(rf_jpm_test)
rf_jpm_test <- create_targets(rf_jpm_test)

# Remove NA values
rf_jpm_train <- na.omit(rf_jpm_train)
rf_jpm_test <- na.omit(rf_jpm_test)

# Define features
rf_features <- c("ret_lag1", "ret_lag2", "ret_lag3",
                "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
                "vol_momentum_5d", "vol_momentum_ratio",
                "abs_ret", "avg_abs_ret_5d",
                "mkt_vol_5d", "mkt_vol_10d",
                "market_rel_vol", "vol_trend_3d",
                "vol_ratio_5_22", "range_5d",
                "vol_range_ratio", "momentum_range_interact",
                "vol_market_interact", "range_momentum_ratio")

# Train initial models to select important features
initial_rf_reg <- randomForest(next_vol ~ .,
                             data = rf_jpm_train[, c(rf_features, "next_vol")],
                             ntree = 100,
                             importance = TRUE)

initial_rf_class <- randomForest(vol_direction ~ .,
                               data = rf_jpm_train[, c(rf_features, "vol_direction")],
                               ntree = 100,
                               importance = TRUE)

# Select significant features (>5% of max importance)
reg_importance <- importance(initial_rf_reg)[,1]
class_importance <- importance(initial_rf_class)[,3]

reg_threshold <- 0.05 * max(reg_importance)
class_threshold <- 0.05 * max(class_importance)

significant_reg_features <- names(which(reg_importance > reg_threshold))
significant_class_features <- names(which(class_importance > class_threshold))

# Train final models with selected features
rf_model_reg <- randomForest(next_vol ~ .,
                           data = rf_jpm_train[, c(significant_reg_features, "next_vol")],
                           ntree = 500,
                           maxnodes = 30,
                           nodesize = 10,
                           importance = TRUE)

rf_model_class <- randomForest(vol_direction ~ .,
                             data = rf_jpm_train[, c(significant_class_features, "vol_direction")],
                             ntree = 500,
                             maxnodes = 30,
                             nodesize = 10,
                             importance = TRUE)

# Make predictions
train_pred_class <- predict(rf_model_class, rf_jpm_train)
test_pred_class <- predict(rf_model_class, rf_jpm_test)
train_pred_prob <- predict(rf_model_class, rf_jpm_train, type="prob")
test_pred_prob <- predict(rf_model_class, rf_jpm_test, type="prob")

train_pred_reg <- predict(rf_model_reg, rf_jpm_train)
test_pred_reg <- predict(rf_model_reg, rf_jpm_test)

# Calculate metrics
train_conf <- table(Predicted = train_pred_class, 
                   Actual = rf_jpm_train$vol_direction)
test_conf <- table(Predicted = test_pred_class, 
                  Actual = rf_jpm_test$vol_direction)

train_acc <- sum(diag(train_conf)) / sum(train_conf)
test_acc <- sum(diag(test_conf)) / sum(test_conf)

roc_obj <- roc(rf_jpm_test$vol_direction, test_pred_prob[,2])
auc_value <- auc(roc_obj)

roc_jpm_rf <- roc_obj

train_rmse <- sqrt(mean((rf_jpm_train$next_vol - train_pred_reg)^2))
test_rmse <- sqrt(mean((rf_jpm_test$next_vol - test_pred_reg)^2))
train_mae <- mean(abs(rf_jpm_train$next_vol - train_pred_reg))
test_mae <- mean(abs(rf_jpm_test$next_vol - test_pred_reg))

# Print results
cat("\nSelected Features for Classification Model:\n")
print(significant_class_features)

cat("\nSelected Features for Regression Model:\n")
print(significant_reg_features)

cat("\nClassification Results:\n")
cat("Training Accuracy:", round(train_acc * 100, 2), "%\n")
cat("Testing Accuracy:", round(test_acc * 100, 2), "%\n")
cat("AUC:", round(auc_value, 3), "\n")

cat("\nRegression Results:\n")
cat("Training RMSE:", round(train_rmse, 6), "\n")
cat("Testing RMSE:", round(test_rmse, 6), "\n")
cat("Training MAE:", round(train_mae, 6), "\n")
cat("Testing MAE:", round(test_mae, 6), "\n")

# Plotting
par(mfrow=c(2,2))

# ROC curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Variable importance plots for both models
varImpPlot(rf_model_class, 
          main="Variable Importance (Classification)",
          color="darkblue")
varImpPlot(rf_model_reg, 
          main="Variable Importance (Regression)",
          color="darkred")

# Compare actual vs predicted volatility
plot(rf_jpm_test$next_vol, type="l", col="blue", 
     main="Actual vs Predicted Volatility (Test Set)",
     xlab="Time", ylab="Volatility")
lines(test_pred_reg, col="red")
legend("topright", legend=c("Actual", "Predicted"), 
       col=c("blue", "red"), lty=1)
```


***RF Apple***

```{r}
library(pROC)
set.seed(201)  # For general R operations
RNGkind(sample.kind = "Rounding") 

# Create copies of training and test data
rf_apple_train <- apple_train
rf_apple_test <- apple_test

# Enhanced Feature Engineering Function with interactions
create_rf_features <- function(data) {
    df <- data.frame(data)
    
    # Base features
    df$ret_lag1 <- lag(df$RET, 1)
    df$ret_lag2 <- lag(df$RET, 2)
    df$ret_lag3 <- lag(df$RET, 3)
    
    # Volatility features
    df$roll_vol_5d <- rollapply(df$RET, width=5, FUN=sd, fill=NA, align="right")
    df$roll_vol_10d <- rollapply(df$RET, width=10, FUN=sd, fill=NA, align="right")
    df$roll_vol_22d <- rollapply(df$RET, width=22, FUN=sd, fill=NA, align="right")
    
    # Volatility momentum
    df$vol_momentum_5d <- df$roll_vol_5d - lag(df$roll_vol_5d, 5)
    df$vol_momentum_ratio <- df$vol_momentum_5d / df$roll_vol_5d
    
    # Return patterns
    df$abs_ret <- abs(df$RET)
    df$avg_abs_ret_5d <- rollapply(abs(df$RET), width=5, FUN=mean, fill=NA, align="right")
    
    # Market features
    df$mkt_vol_5d <- rollapply(df$sprtrn, width=5, FUN=sd, fill=NA, align="right")
    df$mkt_vol_10d <- rollapply(df$sprtrn, width=10, FUN=sd, fill=NA, align="right")
    
    # Additional features
    df$market_rel_vol <- df$roll_vol_5d / df$mkt_vol_5d
    df$vol_trend_3d <- rollapply(df$roll_vol_5d, 3, 
                                function(x) (x[3] - x[1])/x[1], 
                                fill=NA, align="right")
    df$vol_ratio_5_22 <- df$roll_vol_5d / df$roll_vol_22d
    df$range_5d <- rollapply(df$RET, width=5, 
                            FUN=function(x) max(x) - min(x), 
                            fill=NA, align="right")
    
    # Feature interactions
    df$vol_range_ratio <- df$range_5d / df$roll_vol_5d
    df$momentum_range_interact <- df$vol_momentum_5d * df$range_5d
    df$vol_market_interact <- df$roll_vol_5d * df$mkt_vol_5d
    df$range_momentum_ratio <- df$range_5d / (df$vol_momentum_5d + 1e-6)
    
    return(df)
}

# Create targets
create_targets <- function(data) {
    df <- data.frame(data)
    # Calculate forward-looking 5-day volatility (annualized)
    current_vol <- rollapply(df$RET, width=5, FUN=sd, fill=NA, align="right") * sqrt(252)
    df$next_vol <- rollapply(df$RET, width=5, FUN=sd, fill=NA, align="left") * sqrt(252)
    df$vol_direction <- factor(ifelse(df$next_vol > current_vol, 1, 0))
    return(df)
}

# Prepare data
rf_apple_train <- create_rf_features(rf_apple_train)
rf_apple_train <- create_targets(rf_apple_train)
rf_apple_test <- create_rf_features(rf_apple_test)
rf_apple_test <- create_targets(rf_apple_test)

# Remove NA values
rf_apple_train <- na.omit(rf_apple_train)
rf_apple_test <- na.omit(rf_apple_test)

# Define features
rf_features <- c("ret_lag1", "ret_lag2", "ret_lag3",
                "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
                "vol_momentum_5d", "vol_momentum_ratio",
                "abs_ret", "avg_abs_ret_5d",
                "mkt_vol_5d", "mkt_vol_10d",
                "market_rel_vol", "vol_trend_3d",
                "vol_ratio_5_22", "range_5d",
                "vol_range_ratio", "momentum_range_interact",
                "vol_market_interact", "range_momentum_ratio")

# Train initial models to select important features
initial_rf_reg <- randomForest(next_vol ~ .,
                             data = rf_apple_train[, c(rf_features, "next_vol")],
                             ntree = 100,
                             importance = TRUE)

initial_rf_class <- randomForest(vol_direction ~ .,
                               data = rf_apple_train[, c(rf_features, "vol_direction")],
                               ntree = 100,
                               importance = TRUE)

# Select significant features (>5% of max importance)
reg_importance <- importance(initial_rf_reg)[,1]
class_importance <- importance(initial_rf_class)[,3]

reg_threshold <- 0.05 * max(reg_importance)
class_threshold <- 0.05 * max(class_importance)

significant_reg_features <- names(which(reg_importance > reg_threshold))
significant_class_features <- names(which(class_importance > class_threshold))

# Train final models with selected features
rf_model_reg <- randomForest(next_vol ~ .,
                           data = rf_apple_train[, c(significant_reg_features, "next_vol")],
                           ntree = 500,
                           maxnodes = 30,
                           nodesize = 10,
                           importance = TRUE)

rf_model_class <- randomForest(vol_direction ~ .,
                             data = rf_apple_train[, c(significant_class_features, "vol_direction")],
                             ntree = 500,
                             maxnodes = 30,
                             nodesize = 10,
                             importance = TRUE)

# Make predictions
train_pred_class <- predict(rf_model_class, rf_apple_train)
test_pred_class <- predict(rf_model_class, rf_apple_test)
train_pred_prob <- predict(rf_model_class, rf_apple_train, type="prob")
test_pred_prob <- predict(rf_model_class, rf_apple_test, type="prob")

train_pred_reg <- predict(rf_model_reg, rf_apple_train)
test_pred_reg <- predict(rf_model_reg, rf_apple_test)

# Calculate metrics
train_conf <- table(Predicted = train_pred_class, 
                   Actual = rf_apple_train$vol_direction)
test_conf <- table(Predicted = test_pred_class, 
                  Actual = rf_apple_test$vol_direction)

train_acc <- sum(diag(train_conf)) / sum(train_conf)
test_acc <- sum(diag(test_conf)) / sum(test_conf)

roc_obj <- roc(rf_apple_test$vol_direction, test_pred_prob[,2])
auc_value <- auc(roc_obj)

# Save the ROC object for later comparison
roc_apple_rf <- roc_obj

train_rmse <- sqrt(mean((rf_apple_train$next_vol - train_pred_reg)^2))
test_rmse <- sqrt(mean((rf_apple_test$next_vol - test_pred_reg)^2))
train_mae <- mean(abs(rf_apple_train$next_vol - train_pred_reg))
test_mae <- mean(abs(rf_apple_test$next_vol - test_pred_reg))

# Print results
cat("\nSelected Features for Classification Model:\n")
print(significant_class_features)

cat("\nSelected Features for Regression Model:\n")
print(significant_reg_features)

cat("\nClassification Results:\n")
cat("Training Accuracy:", round(train_acc * 100, 2), "%\n")
cat("Testing Accuracy:", round(test_acc * 100, 2), "%\n")
cat("AUC:", round(auc_value, 3), "\n")

cat("\nRegression Results:\n")
cat("Training RMSE:", round(train_rmse, 6), "\n")
cat("Testing RMSE:", round(test_rmse, 6), "\n")
cat("Training MAE:", round(train_mae, 6), "\n")
cat("Testing MAE:", round(test_mae, 6), "\n")

# Plotting
par(mfrow=c(2,2))

# ROC curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Variable importance for both models
varImpPlot(rf_model_class, 
          main="Variable Importance (Classification)",
          color="darkblue")
varImpPlot(rf_model_reg, 
          main="Variable Importance (Regression)",
          color="darkred")

# Compare actual vs predicted volatility
plot(rf_apple_test$next_vol, type="l", col="blue", 
     main="Actual vs Predicted Volatility (Test Set)",
     xlab="Time", ylab="Volatility")
lines(test_pred_reg, col="red")
legend("topright", legend=c("Actual", "Predicted"), 
       col=c("blue", "red"), lty=1)
```
***Tesla RF***

```{r}
library(pROC)
set.seed(201)  # For general R operations
RNGkind(sample.kind = "Rounding") 

# Create copies of training and test data
rf_tesla_train <- tesla_train
rf_tesla_test <- tesla_test

# Enhanced Feature Engineering Function with interactions
create_rf_features <- function(data) {
    df <- data.frame(data)
    
    # Base features
    df$ret_lag1 <- lag(df$RET, 1)
    df$ret_lag2 <- lag(df$RET, 2)
    df$ret_lag3 <- lag(df$RET, 3)
    
    # Volatility features
    df$roll_vol_5d <- rollapply(df$RET, width=5, FUN=sd, fill=NA, align="right")
    df$roll_vol_10d <- rollapply(df$RET, width=10, FUN=sd, fill=NA, align="right")
    df$roll_vol_22d <- rollapply(df$RET, width=22, FUN=sd, fill=NA, align="right")
    
    # Volatility momentum
    df$vol_momentum_5d <- df$roll_vol_5d - lag(df$roll_vol_5d, 5)
    df$vol_momentum_ratio <- df$vol_momentum_5d / df$roll_vol_5d
    
    # Return patterns
    df$abs_ret <- abs(df$RET)
    df$avg_abs_ret_5d <- rollapply(abs(df$RET), width=5, FUN=mean, fill=NA, align="right")
    
    # Market features
    df$mkt_vol_5d <- rollapply(df$sprtrn, width=5, FUN=sd, fill=NA, align="right")
    df$mkt_vol_10d <- rollapply(df$sprtrn, width=10, FUN=sd, fill=NA, align="right")
    
    # Additional features
    df$market_rel_vol <- df$roll_vol_5d / df$mkt_vol_5d
    df$vol_trend_3d <- rollapply(df$roll_vol_5d, 3, 
                                function(x) (x[3] - x[1])/x[1], 
                                fill=NA, align="right")
    df$vol_ratio_5_22 <- df$roll_vol_5d / df$roll_vol_22d
    df$range_5d <- rollapply(df$RET, width=5, 
                            FUN=function(x) max(x) - min(x), 
                            fill=NA, align="right")
    
    # Feature interactions
    df$vol_range_ratio <- df$range_5d / df$roll_vol_5d
    df$momentum_range_interact <- df$vol_momentum_5d * df$range_5d
    df$vol_market_interact <- df$roll_vol_5d * df$mkt_vol_5d
    df$range_momentum_ratio <- df$range_5d / (df$vol_momentum_5d + 1e-6)
    
    return(df)
}

# Create targets
create_targets <- function(data) {
    df <- data.frame(data)
    # Calculate forward-looking 5-day volatility (annualized)
    current_vol <- rollapply(df$RET, width=5, FUN=sd, fill=NA, align="right") * sqrt(252)
    df$next_vol <- rollapply(df$RET, width=5, FUN=sd, fill=NA, align="left") * sqrt(252)
    df$vol_direction <- factor(ifelse(df$next_vol > current_vol, 1, 0))
    return(df)
}

# Prepare data
rf_tesla_train <- create_rf_features(rf_tesla_train)
rf_tesla_train <- create_targets(rf_tesla_train)
rf_tesla_test <- create_rf_features(rf_tesla_test)
rf_tesla_test <- create_targets(rf_tesla_test)

# Remove NA values
rf_tesla_train <- na.omit(rf_tesla_train)
rf_tesla_test <- na.omit(rf_tesla_test)

# Define features
rf_features <- c("ret_lag1", "ret_lag2", "ret_lag3",
                "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
                "vol_momentum_5d", "vol_momentum_ratio",
                "abs_ret", "avg_abs_ret_5d",
                "mkt_vol_5d", "mkt_vol_10d",
                "market_rel_vol", "vol_trend_3d",
                "vol_ratio_5_22", "range_5d",
                "vol_range_ratio", "momentum_range_interact",
                "vol_market_interact", "range_momentum_ratio")

# Train initial models to select important features
initial_rf_reg <- randomForest(next_vol ~ .,
                             data = rf_tesla_train[, c(rf_features, "next_vol")],
                             ntree = 100,
                             importance = TRUE)

initial_rf_class <- randomForest(vol_direction ~ .,
                               data = rf_tesla_train[, c(rf_features, "vol_direction")],
                               ntree = 100,
                               importance = TRUE)

# Select significant features (>5% of max importance)
reg_importance <- importance(initial_rf_reg)[,1]
class_importance <- importance(initial_rf_class)[,3]

reg_threshold <- 0.05 * max(reg_importance)
class_threshold <- 0.05 * max(class_importance)

significant_reg_features <- names(which(reg_importance > reg_threshold))
significant_class_features <- names(which(class_importance > class_threshold))

# Train final models with selected features
rf_model_reg <- randomForest(next_vol ~ .,
                           data = rf_tesla_train[, c(significant_reg_features, "next_vol")],
                           ntree = 500,
                           maxnodes = 30,
                           nodesize = 10,
                           importance = TRUE)

rf_model_class <- randomForest(vol_direction ~ .,
                             data = rf_tesla_train[, c(significant_class_features, "vol_direction")],
                             ntree = 500,
                             maxnodes = 30,
                             nodesize = 10,
                             importance = TRUE)

# Make predictions
train_pred_class <- predict(rf_model_class, rf_tesla_train)
test_pred_class <- predict(rf_model_class, rf_tesla_test)
train_pred_prob <- predict(rf_model_class, rf_tesla_train, type="prob")
test_pred_prob <- predict(rf_model_class, rf_tesla_test, type="prob")

train_pred_reg <- predict(rf_model_reg, rf_tesla_train)
test_pred_reg <- predict(rf_model_reg, rf_tesla_test)

# Calculate metrics
train_conf <- table(Predicted = train_pred_class, 
                   Actual = rf_tesla_train$vol_direction)
test_conf <- table(Predicted = test_pred_class, 
                  Actual = rf_tesla_test$vol_direction)

train_acc <- sum(diag(train_conf)) / sum(train_conf)
test_acc <- sum(diag(test_conf)) / sum(test_conf)

roc_obj <- roc(rf_tesla_test$vol_direction, test_pred_prob[,2])
auc_value <- auc(roc_obj)

# Save the ROC object for later comparison
roc_tesla_rf <- roc_obj

train_rmse <- sqrt(mean((rf_tesla_train$next_vol - train_pred_reg)^2))
test_rmse <- sqrt(mean((rf_tesla_test$next_vol - test_pred_reg)^2))
train_mae <- mean(abs(rf_tesla_train$next_vol - train_pred_reg))
test_mae <- mean(abs(rf_tesla_test$next_vol - test_pred_reg))

# Print results
cat("\nSelected Features for Classification Model:\n")
print(significant_class_features)

cat("\nSelected Features for Regression Model:\n")
print(significant_reg_features)

cat("\nClassification Results:\n")
cat("Training Accuracy:", round(train_acc * 100, 2), "%\n")
cat("Testing Accuracy:", round(test_acc * 100, 2), "%\n")
cat("AUC:", round(auc_value, 3), "\n")

cat("\nRegression Results:\n")
cat("Training RMSE:", round(train_rmse, 6), "\n")
cat("Testing RMSE:", round(test_rmse, 6), "\n")
cat("Training MAE:", round(train_mae, 6), "\n")
cat("Testing MAE:", round(test_mae, 6), "\n")

# Plotting
par(mfrow=c(2,2))

# ROC curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Variable importance for both models
varImpPlot(rf_model_class, 
          main="Variable Importance (Classification)",
          color="darkblue")
varImpPlot(rf_model_reg, 
          main="Variable Importance (Regression)",
          color="darkred")

# Compare actual vs predicted volatility
plot(rf_tesla_test$next_vol, type="l", col="blue", 
     main="Actual vs Predicted Volatility (Test Set)",
     xlab="Time", ylab="Volatility")
lines(test_pred_reg, col="red")
legend("topright", legend=c("Actual", "Predicted"), 
       col=c("blue", "red"), lty=1)
```



## GBM Model (XGboost) for JPM
same feature set as RF and all other models
```{r}
# Load required libraries
library(xgboost)
library(dplyr)
library(zoo)
library(ggplot2)
library(gridExtra)
library(scales)
library(pROC)

# Set seed for reproducibility
set.seed(201)
RNGkind(sample.kind = "Rounding")

# Create RF-consistent feature engineering function
create_rf_features <- function(data) {
    data %>%
        arrange(date) %>%
        mutate(
            # Base return features
            ret_lag1 = lag(RET, 1),
            ret_lag2 = lag(RET, 2),
            ret_lag3 = lag(RET, 3),
            
            # Volatility features
            roll_vol_5d = rollapply(RET, width=5, FUN=sd, fill=NA, align="right"),
            roll_vol_10d = rollapply(RET, width=10, FUN=sd, fill=NA, align="right"),
            roll_vol_22d = rollapply(RET, width=22, FUN=sd, fill=NA, align="right"),
            
            # Volatility momentum
            vol_momentum_5d = roll_vol_5d - lag(roll_vol_5d, 5),
            vol_momentum_ratio = vol_momentum_5d / roll_vol_5d,
            
            # Return patterns
            abs_ret = abs(RET),
            avg_abs_ret_5d = rollapply(abs(RET), width=5, FUN=mean, fill=NA, align="right"),
            
            # Market features
            mkt_vol_5d = rollapply(sprtrn, width=5, FUN=sd, fill=NA, align="right"),
            mkt_vol_10d = rollapply(sprtrn, width=10, FUN=sd, fill=NA, align="right"),
            
            # Additional features
            market_rel_vol = roll_vol_5d / mkt_vol_5d,
            vol_trend_3d = rollapply(roll_vol_5d, 3, 
                                   function(x) (x[3] - x[1])/x[1], 
                                   fill=NA, align="right"),
            vol_ratio_5_22 = roll_vol_5d / roll_vol_22d,
            range_5d = rollapply(RET, width=5, 
                               FUN=function(x) max(x) - min(x), 
                               fill=NA, align="right"),
            
            # Feature interactions
            vol_range_ratio = range_5d / roll_vol_5d,
            momentum_range_interact = vol_momentum_5d * range_5d,
            vol_market_interact = roll_vol_5d * mkt_vol_5d,
            range_momentum_ratio = range_5d / (vol_momentum_5d + 1e-8),
            
            # Target (5-day forward volatility)
            target_vol = rollapply(RET, width=5, 
                                 FUN=function(x) sd(x), 
                                 align="left", 
                                 fill=NA) * sqrt(252),
            
            # Current volatility for direction comparison
            current_vol = rollapply(RET, width=5, 
                                  FUN=function(x) sd(x), 
                                  align="right", 
                                  fill=NA) * sqrt(252)
        )
}

# Prepare features for training and testing sets
train_features <- create_rf_features(jpm_train)
test_features <- create_rf_features(jpm_test)

# Define feature columns
feature_cols <- c(
    "ret_lag1", "ret_lag2", "ret_lag3",
    "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
    "vol_momentum_5d", "vol_momentum_ratio",
    "abs_ret", "avg_abs_ret_5d",
    "mkt_vol_5d", "mkt_vol_10d",
    "market_rel_vol", "vol_trend_3d",
    "vol_ratio_5_22", "range_5d",
    "vol_range_ratio", "momentum_range_interact",
    "vol_market_interact", "range_momentum_ratio"
)

# Clean the data
train_features <- na.omit(train_features)
test_features <- na.omit(test_features)

# Create clean X and y matrices
X_train <- as.matrix(train_features[, feature_cols])
X_test <- as.matrix(test_features[, feature_cols])
y_train <- train_features$target_vol
y_test <- test_features$target_vol

# Handle any infinite values
X_train[is.infinite(X_train)] <- NA
X_test[is.infinite(X_test)] <- NA
train_features <- na.omit(train_features)
test_features <- na.omit(test_features)
X_train <- as.matrix(train_features[, feature_cols])
X_test <- as.matrix(test_features[, feature_cols])
y_train <- train_features$target_vol
y_test <- test_features$target_vol

# Create DMatrix objects
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)

# Set XGBoost parameters
params <- list(
    objective = "reg:squarederror",
    eval_metric = "rmse",
    max_depth = 5,
    eta = 0.01,
    subsample = 0.8,
    colsample_bytree = 0.8,
    min_child_weight = 5,
    gamma = 0,
    lambda = 1,
    alpha = 0
)

# Train model
cv_results <- xgb.cv(
    params = params,
    data = dtrain,
    nfold = 5,
    nrounds = 1000,
    early_stopping_rounds = 50,
    verbose = 1,
    metrics = "rmse"
)

# Get optimal number of rounds
optimal_nrounds <- which.min(cv_results$evaluation_log$test_rmse_mean)

# Train final model
final_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = optimal_nrounds,
    watchlist = list(train = dtrain, test = dtest),
    verbose = 1
)

# Make predictions
final_predictions <- predict(final_model, dtest)

# Calculate performance metrics
final_rmse <- sqrt(mean((y_test - final_predictions)^2))
final_mae <- mean(abs(y_test - final_predictions))
final_r_squared <- 1 - sum((y_test - final_predictions)^2) / sum((y_test - mean(y_test))^2)

# Calculate directional accuracy
actual_direction <- as.factor(ifelse(test_features$target_vol > test_features$current_vol, 1, 0))
predicted_direction <- as.factor(ifelse(final_predictions > test_features$current_vol, 1, 0))
direction_accuracy <- mean(predicted_direction == actual_direction, na.rm = TRUE)

# Calculate probabilities for ROC curve (scale predictions to [0,1] range)
scaled_predictions <- (final_predictions - min(final_predictions)) / 
                     (max(final_predictions) - min(final_predictions))

# Calculate ROC and AUC
roc_obj <- roc(actual_direction, scaled_predictions)
auc_value <- auc(roc_obj)

roc_jpm_gbm <- roc_obj

# Create confusion matrix
conf_matrix <- table(Predicted = predicted_direction, Actual = actual_direction)

# Print results
cat("\nModel Performance Metrics:\n")
cat("RMSE:", round(final_rmse, 6), "\n")
cat("MAE:", round(final_mae, 6), "\n")
cat("R-squared:", round(final_r_squared, 4), "\n")
cat("Directional Accuracy:", round(direction_accuracy * 100, 2), "%\n")
cat("AUC:", round(auc_value, 3), "\n")

cat("\nConfusion Matrix:\n")
print(conf_matrix)

# Create visualization data
plot_data <- data.frame(
    Date = test_features$date,
    Actual = y_test,
    Predicted = final_predictions
)

# Set up plotting layout
par(mfrow=c(2,2))

# Plot 1: ROC Curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Plot 2: Actual vs Predicted Volatility Time Series
plot(plot_data$Actual, type="l", col="blue", 
     main="Actual vs Predicted Volatility (Test Set)",
     xlab="Time", ylab="Volatility")
lines(plot_data$Predicted, col="red")
legend("topright", legend=c("Actual", "Predicted"), 
       col=c("blue", "red"), lty=1)

# Plot 3: Feature Importance
importance_matrix <- xgb.importance(feature_names = feature_cols, model = final_model)
xgb.plot.importance(importance_matrix[1:10,], main = "Top 10 Features")

# Plot 4: Predicted vs Actual Scatter Plot
plot(plot_data$Actual, plot_data$Predicted,
     main="Predicted vs Actual Volatility",
     xlab="Actual Volatility",
     ylab="Predicted Volatility")
abline(0, 1, col="red", lty=2)

# Print feature importance
print("\nFeature Importance:")
print(importance_matrix)
```

***Apple GBM***

```{r}
# Load required libraries
library(xgboost)
library(dplyr)
library(zoo)
library(ggplot2)
library(gridExtra)
library(scales)
library(pROC)

# Set seed for reproducibility
set.seed(201)
RNGkind(sample.kind = "Rounding")

# Create RF-consistent feature engineering function
create_rf_features <- function(data) {
    data %>%
        arrange(date) %>%
        mutate(
            # Base return features
            ret_lag1 = lag(RET, 1),
            ret_lag2 = lag(RET, 2),
            ret_lag3 = lag(RET, 3),
            
            # Volatility features
            roll_vol_5d = rollapply(RET, width=5, FUN=sd, fill=NA, align="right"),
            roll_vol_10d = rollapply(RET, width=10, FUN=sd, fill=NA, align="right"),
            roll_vol_22d = rollapply(RET, width=22, FUN=sd, fill=NA, align="right"),
            
            # Volatility momentum
            vol_momentum_5d = roll_vol_5d - lag(roll_vol_5d, 5),
            vol_momentum_ratio = vol_momentum_5d / roll_vol_5d,
            
            # Return patterns
            abs_ret = abs(RET),
            avg_abs_ret_5d = rollapply(abs(RET), width=5, FUN=mean, fill=NA, align="right"),
            
            # Market features
            mkt_vol_5d = rollapply(sprtrn, width=5, FUN=sd, fill=NA, align="right"),
            mkt_vol_10d = rollapply(sprtrn, width=10, FUN=sd, fill=NA, align="right"),
            
            # Additional features
            market_rel_vol = roll_vol_5d / mkt_vol_5d,
            vol_trend_3d = rollapply(roll_vol_5d, 3, 
                                   function(x) (x[3] - x[1])/x[1], 
                                   fill=NA, align="right"),
            vol_ratio_5_22 = roll_vol_5d / roll_vol_22d,
            range_5d = rollapply(RET, width=5, 
                               FUN=function(x) max(x) - min(x), 
                               fill=NA, align="right"),
            
            # Feature interactions
            vol_range_ratio = range_5d / roll_vol_5d,
            momentum_range_interact = vol_momentum_5d * range_5d,
            vol_market_interact = roll_vol_5d * mkt_vol_5d,
            range_momentum_ratio = range_5d / (vol_momentum_5d + 1e-8),
            
            # Target (5-day forward volatility)
            target_vol = rollapply(RET, width=5, 
                                 FUN=function(x) sd(x), 
                                 align="left", 
                                 fill=NA) * sqrt(252),
            
            # Current volatility for direction comparison
            current_vol = rollapply(RET, width=5, 
                                  FUN=function(x) sd(x), 
                                  align="right", 
                                  fill=NA) * sqrt(252)
        )
}

# Prepare features for training and testing sets
train_features <- create_rf_features(apple_train)
test_features <- create_rf_features(apple_test)

# Define feature columns
feature_cols <- c(
    "ret_lag1", "ret_lag2", "ret_lag3",
    "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
    "vol_momentum_5d", "vol_momentum_ratio",
    "abs_ret", "avg_abs_ret_5d",
    "mkt_vol_5d", "mkt_vol_10d",
    "market_rel_vol", "vol_trend_3d",
    "vol_ratio_5_22", "range_5d",
    "vol_range_ratio", "momentum_range_interact",
    "vol_market_interact", "range_momentum_ratio"
)

# Clean the data
train_features <- na.omit(train_features)
test_features <- na.omit(test_features)

# Create clean X and y matrices
X_train <- as.matrix(train_features[, feature_cols])
X_test <- as.matrix(test_features[, feature_cols])
y_train <- train_features$target_vol
y_test <- test_features$target_vol

# Handle any infinite values
X_train[is.infinite(X_train)] <- NA
X_test[is.infinite(X_test)] <- NA
train_features <- na.omit(train_features)
test_features <- na.omit(test_features)
X_train <- as.matrix(train_features[, feature_cols])
X_test <- as.matrix(test_features[, feature_cols])
y_train <- train_features$target_vol
y_test <- test_features$target_vol

# Create DMatrix objects
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)

# Set XGBoost parameters
params <- list(
    objective = "reg:squarederror",
    eval_metric = "rmse",
    max_depth = 5,
    eta = 0.01,
    subsample = 0.8,
    colsample_bytree = 0.8,
    min_child_weight = 5,
    gamma = 0,
    lambda = 1,
    alpha = 0
)

# Train model
cv_results <- xgb.cv(
    params = params,
    data = dtrain,
    nfold = 5,
    nrounds = 1000,
    early_stopping_rounds = 50,
    verbose = 1,
    metrics = "rmse"
)

# Get optimal number of rounds
optimal_nrounds <- which.min(cv_results$evaluation_log$test_rmse_mean)

# Train final model
final_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = optimal_nrounds,
    watchlist = list(train = dtrain, test = dtest),
    verbose = 1
)

# Make predictions
final_predictions <- predict(final_model, dtest)

# Calculate performance metrics
final_rmse <- sqrt(mean((y_test - final_predictions)^2))
final_mae <- mean(abs(y_test - final_predictions))
final_r_squared <- 1 - sum((y_test - final_predictions)^2) / sum((y_test - mean(y_test))^2)

# Calculate directional accuracy
actual_direction <- as.factor(ifelse(test_features$target_vol > test_features$current_vol, 1, 0))
predicted_direction <- as.factor(ifelse(final_predictions > test_features$current_vol, 1, 0))
direction_accuracy <- mean(predicted_direction == actual_direction, na.rm = TRUE)

# Calculate probabilities for ROC curve (scale predictions to [0,1] range)
scaled_predictions <- (final_predictions - min(final_predictions)) / 
                     (max(final_predictions) - min(final_predictions))

# Calculate ROC and AUC
roc_obj <- roc(actual_direction, scaled_predictions)
auc_value <- auc(roc_obj)

# Save the ROC object for later comparison
roc_apple_gbm <- roc_obj

# Create confusion matrix
conf_matrix <- table(Predicted = predicted_direction, Actual = actual_direction)

# Print results
cat("\nModel Performance Metrics:\n")
cat("RMSE:", round(final_rmse, 6), "\n")
cat("MAE:", round(final_mae, 6), "\n")
cat("R-squared:", round(final_r_squared, 4), "\n")
cat("Directional Accuracy:", round(direction_accuracy * 100, 2), "%\n")
cat("AUC:", round(auc_value, 3), "\n")

cat("\nConfusion Matrix:\n")
print(conf_matrix)

# Create visualization data
plot_data <- data.frame(
    Date = test_features$date,
    Actual = y_test,
    Predicted = final_predictions
)

# Set up plotting layout
par(mfrow=c(2,2))

# Plot 1: ROC Curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Plot 2: Actual vs Predicted Volatility Time Series
plot(plot_data$Actual, type="l", col="blue", 
     main="Actual vs Predicted Volatility (Test Set)",
     xlab="Time", ylab="Volatility")
lines(plot_data$Predicted, col="red")
legend("topright", legend=c("Actual", "Predicted"), 
       col=c("blue", "red"), lty=1)

# Plot 3: Feature Importance
importance_matrix <- xgb.importance(feature_names = feature_cols, model = final_model)
xgb.plot.importance(importance_matrix[1:10,], main = "Top 10 Features")

# Plot 4: Predicted vs Actual Scatter Plot
plot(plot_data$Actual, plot_data$Predicted,
     main="Predicted vs Actual Volatility",
     xlab="Actual Volatility",
     ylab="Predicted Volatility")
abline(0, 1, col="red", lty=2)

# Print feature importance
print("\nFeature Importance:")
print(importance_matrix)
```
***Tesla GBM***

```{r}
# Load required libraries
library(xgboost)
library(dplyr)
library(zoo)
library(ggplot2)
library(gridExtra)
library(scales)
library(pROC)

# Set seed for reproducibility
set.seed(201)
RNGkind(sample.kind = "Rounding")

# Create RF-consistent feature engineering function
create_rf_features <- function(data) {
    data %>%
        arrange(date) %>%
        mutate(
            # Base return features
            ret_lag1 = lag(RET, 1),
            ret_lag2 = lag(RET, 2),
            ret_lag3 = lag(RET, 3),
            
            # Volatility features
            roll_vol_5d = rollapply(RET, width=5, FUN=sd, fill=NA, align="right"),
            roll_vol_10d = rollapply(RET, width=10, FUN=sd, fill=NA, align="right"),
            roll_vol_22d = rollapply(RET, width=22, FUN=sd, fill=NA, align="right"),
            
            # Volatility momentum
            vol_momentum_5d = roll_vol_5d - lag(roll_vol_5d, 5),
            vol_momentum_ratio = vol_momentum_5d / roll_vol_5d,
            
            # Return patterns
            abs_ret = abs(RET),
            avg_abs_ret_5d = rollapply(abs(RET), width=5, FUN=mean, fill=NA, align="right"),
            
            # Market features
            mkt_vol_5d = rollapply(sprtrn, width=5, FUN=sd, fill=NA, align="right"),
            mkt_vol_10d = rollapply(sprtrn, width=10, FUN=sd, fill=NA, align="right"),
            
            # Additional features
            market_rel_vol = roll_vol_5d / mkt_vol_5d,
            vol_trend_3d = rollapply(roll_vol_5d, 3, 
                                   function(x) (x[3] - x[1])/x[1], 
                                   fill=NA, align="right"),
            vol_ratio_5_22 = roll_vol_5d / roll_vol_22d,
            range_5d = rollapply(RET, width=5, 
                               FUN=function(x) max(x) - min(x), 
                               fill=NA, align="right"),
            
            # Feature interactions
            vol_range_ratio = range_5d / roll_vol_5d,
            momentum_range_interact = vol_momentum_5d * range_5d,
            vol_market_interact = roll_vol_5d * mkt_vol_5d,
            range_momentum_ratio = range_5d / (vol_momentum_5d + 1e-8),
            
            # Target (5-day forward volatility)
            target_vol = rollapply(RET, width=5, 
                                 FUN=function(x) sd(x), 
                                 align="left", 
                                 fill=NA) * sqrt(252),
            
            # Current volatility for direction comparison
            current_vol = rollapply(RET, width=5, 
                                  FUN=function(x) sd(x), 
                                  align="right", 
                                  fill=NA) * sqrt(252)
        )
}

# Prepare features for training and testing sets
train_features <- create_rf_features(tesla_train)
test_features <- create_rf_features(tesla_test)

# Define feature columns
feature_cols <- c(
    "ret_lag1", "ret_lag2", "ret_lag3",
    "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
    "vol_momentum_5d", "vol_momentum_ratio",
    "abs_ret", "avg_abs_ret_5d",
    "mkt_vol_5d", "mkt_vol_10d",
    "market_rel_vol", "vol_trend_3d",
    "vol_ratio_5_22", "range_5d",
    "vol_range_ratio", "momentum_range_interact",
    "vol_market_interact", "range_momentum_ratio"
)

# Clean the data
train_features <- na.omit(train_features)
test_features <- na.omit(test_features)

# Create clean X and y matrices
X_train <- as.matrix(train_features[, feature_cols])
X_test <- as.matrix(test_features[, feature_cols])
y_train <- train_features$target_vol
y_test <- test_features$target_vol

# Handle any infinite values
X_train[is.infinite(X_train)] <- NA
X_test[is.infinite(X_test)] <- NA
train_features <- na.omit(train_features)
test_features <- na.omit(test_features)
X_train <- as.matrix(train_features[, feature_cols])
X_test <- as.matrix(test_features[, feature_cols])
y_train <- train_features$target_vol
y_test <- test_features$target_vol

# Create DMatrix objects
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)

# Set XGBoost parameters
params <- list(
    objective = "reg:squarederror",
    eval_metric = "rmse",
    max_depth = 5,
    eta = 0.01,
    subsample = 0.8,
    colsample_bytree = 0.8,
    min_child_weight = 5,
    gamma = 0,
    lambda = 1,
    alpha = 0
)

# Train model
cv_results <- xgb.cv(
    params = params,
    data = dtrain,
    nfold = 5,
    nrounds = 1000,
    early_stopping_rounds = 50,
    verbose = 1,
    metrics = "rmse"
)

# Get optimal number of rounds
optimal_nrounds <- which.min(cv_results$evaluation_log$test_rmse_mean)

# Train final model
final_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = optimal_nrounds,
    watchlist = list(train = dtrain, test = dtest),
    verbose = 1
)

# Make predictions
final_predictions <- predict(final_model, dtest)

# Calculate performance metrics
final_rmse <- sqrt(mean((y_test - final_predictions)^2))
final_mae <- mean(abs(y_test - final_predictions))
final_r_squared <- 1 - sum((y_test - final_predictions)^2) / sum((y_test - mean(y_test))^2)

# Calculate directional accuracy
actual_direction <- as.factor(ifelse(test_features$target_vol > test_features$current_vol, 1, 0))
predicted_direction <- as.factor(ifelse(final_predictions > test_features$current_vol, 1, 0))
direction_accuracy <- mean(predicted_direction == actual_direction, na.rm = TRUE)

# Calculate probabilities for ROC curve (scale predictions to [0,1] range)
scaled_predictions <- (final_predictions - min(final_predictions)) / 
                     (max(final_predictions) - min(final_predictions))

# Calculate ROC and AUC
roc_obj <- roc(actual_direction, scaled_predictions)
auc_value <- auc(roc_obj)

# Save the ROC object for later comparison
roc_tesla_gbm <- roc_obj

# Create confusion matrix
conf_matrix <- table(Predicted = predicted_direction, Actual = actual_direction)

# Print results
cat("\nModel Performance Metrics:\n")
cat("RMSE:", round(final_rmse, 6), "\n")
cat("MAE:", round(final_mae, 6), "\n")
cat("R-squared:", round(final_r_squared, 4), "\n")
cat("Directional Accuracy:", round(direction_accuracy * 100, 2), "%\n")
cat("AUC:", round(auc_value, 3), "\n")

cat("\nConfusion Matrix:\n")
print(conf_matrix)

# Create visualization data
plot_data <- data.frame(
    Date = test_features$date,
    Actual = y_test,
    Predicted = final_predictions
)

# Set up plotting layout
par(mfrow=c(2,2))

# Plot 1: ROC Curve
plot(roc_obj, main=paste("ROC Curve (AUC =", round(auc_value, 3), ")"))

# Plot 2: Actual vs Predicted Volatility Time Series
plot(plot_data$Actual, type="l", col="blue", 
     main="Actual vs Predicted Volatility (Test Set)",
     xlab="Time", ylab="Volatility")
lines(plot_data$Predicted, col="red")
legend("topright", legend=c("Actual", "Predicted"), 
       col=c("blue", "red"), lty=1)

# Plot 3: Feature Importance
importance_matrix <- xgb.importance(feature_names = feature_cols, model = final_model)
xgb.plot.importance(importance_matrix[1:10,], main = "Top 10 Features")

# Plot 4: Predicted vs Actual Scatter Plot
plot(plot_data$Actual, plot_data$Predicted,
     main="Predicted vs Actual Volatility",
     xlab="Actual Volatility",
     ylab="Predicted Volatility")
abline(0, 1, col="red", lty=2)

# Print feature importance
print("\nFeature Importance:")
print(importance_matrix)
```





## Neural Network FOR JPM
## FINAL FINAL FINAL FINAL FINAL:
## NEW NN model with the RF smaller variables selection to test performance:
```{r}
library(keras)
library(tidyverse)
library(scales)
library(TTR)
library(pROC)  # Added for ROC curve analysis

# Set seeds for reproducibility 
set.seed(201)
tensorflow::set_random_seed(201)

# Robust scaling function
robust_scale <- function(x) {
  (x - median(x)) / (IQR(x) + 1e-8)
}

# Enhanced data preparation using RF features
prepare_nn_data <- function(train_data, test_data) {
  # Create features consistently with Random Forest implementation
  create_rf_features <- function(data) {
    data %>%
      arrange(date) %>%
      mutate(
        # Target (5-day forward volatility)
        target_vol = rollapply(RET, width=5, FUN=sd, fill=NA, align="left") * sqrt(252),
        
        # Base return features
        ret_lag1 = lag(RET, 1),
        ret_lag2 = lag(RET, 2),
        ret_lag3 = lag(RET, 3),
        
        # Volatility features
        roll_vol_5d = rollapply(RET, width=5, FUN=sd, fill=NA, align="right") * sqrt(252),
        roll_vol_10d = rollapply(RET, width=10, FUN=sd, fill=NA, align="right") * sqrt(252),
        roll_vol_22d = rollapply(RET, width=22, FUN=sd, fill=NA, align="right") * sqrt(252),
        
        # Volatility momentum
        vol_momentum_5d = roll_vol_5d - lag(roll_vol_5d, 5),
        vol_momentum_ratio = vol_momentum_5d / roll_vol_5d,
        
        # Return patterns
        abs_ret = abs(RET),
        avg_abs_ret_5d = rollapply(abs(RET), width=5, FUN=mean, fill=NA, align="right"),
        
        # Market features
        mkt_vol_5d = rollapply(sprtrn, width=5, FUN=sd, fill=NA, align="right") * sqrt(252),
        mkt_vol_10d = rollapply(sprtrn, width=10, FUN=sd, fill=NA, align="right") * sqrt(252),
        
        # Additional features from RF
        market_rel_vol = roll_vol_5d / mkt_vol_5d,
        vol_trend_3d = rollapply(roll_vol_5d, 3, 
                                function(x) (x[3] - x[1])/x[1], 
                                fill=NA, align="right"),
        vol_ratio_5_22 = roll_vol_5d / roll_vol_22d,
        range_5d = rollapply(RET, width=5, 
                            FUN=function(x) max(x) - min(x), 
                            fill=NA, align="right"),
        
        # Feature interactions from RF
        vol_range_ratio = range_5d / roll_vol_5d,
        momentum_range_interact = vol_momentum_5d * range_5d,
        vol_market_interact = roll_vol_5d * mkt_vol_5d,
        range_momentum_ratio = range_5d / (vol_momentum_5d + 1e-8)
    )
  }
  
  # Create features for both datasets
  nn_train <- create_rf_features(train_data)
  nn_test <- create_rf_features(test_data)
  
  # Define features for normalization (all features from RF)
  feature_cols <- c(
    "ret_lag1", "ret_lag2", "ret_lag3",
    "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
    "vol_momentum_5d", "vol_momentum_ratio",
    "abs_ret", "avg_abs_ret_5d",
    "mkt_vol_5d", "mkt_vol_10d",
    "market_rel_vol", "vol_trend_3d",
    "vol_ratio_5_22", "range_5d",
    "vol_range_ratio", "momentum_range_interact",
    "vol_market_interact", "range_momentum_ratio"
  )
  
  # Calculate robust scaling parameters from training data
  scale_params <- lapply(feature_cols, function(col) {
    list(
      median = median(nn_train[[col]], na.rm = TRUE),
      iqr = IQR(nn_train[[col]], na.rm = TRUE)
    )
  })
  names(scale_params) <- feature_cols
  
  # Apply robust scaling
  for(col in feature_cols) {
    nn_train[[paste0(col, "_norm")]] <- (nn_train[[col]] - scale_params[[col]]$median) / 
                                       (scale_params[[col]]$iqr + 1e-8)
    nn_test[[paste0(col, "_norm")]] <- (nn_test[[col]] - scale_params[[col]]$median) / 
                                      (scale_params[[col]]$iqr + 1e-8)
  }
  
  # Remove NA values
  nn_train <- nn_train %>% na.omit()
  nn_test <- nn_test %>% na.omit()
  
  list(
    train = nn_train,
    test = nn_test,
    scale_params = scale_params,
    feature_cols = paste0(feature_cols, "_norm")
  )
}

# Create sequences
create_sequences <- function(data, sequence_length, feature_cols) {
  n <- nrow(data) - sequence_length
  x <- array(0, dim = c(n, sequence_length, length(feature_cols)))
  y <- array(0, dim = c(n))
  
  for(i in 1:n) {
    x[i,,] <- as.matrix(data[i:(i+sequence_length-1), feature_cols])
    y[i] <- data$target_vol[i+sequence_length]
  }
  
  list(x = x, y = y)
}

# Build and train LSTM model
build_lstm_model <- function(prepared_data, sequence_length = 30) {
  train_seq <- create_sequences(prepared_data$train, 
                              sequence_length, 
                              prepared_data$feature_cols)
  test_seq <- create_sequences(prepared_data$test, 
                             sequence_length, 
                             prepared_data$feature_cols)
  
  n_features <- length(prepared_data$feature_cols)
  
  # Define model with optimized architecture
  model <- keras_model_sequential(layers = list(
    # First LSTM block
    layer_lstm(units = 256,
              input_shape = c(sequence_length, n_features),
              return_sequences = TRUE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Second LSTM block
    layer_lstm(units = 128,
              return_sequences = TRUE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Third LSTM block
    layer_lstm(units = 64,
              return_sequences = FALSE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Dense layers
    layer_dense(units = 64, activation = "relu"),
    layer_dropout(rate = 0.3),
    layer_dense(units = 32, activation = "relu"),
    layer_dropout(rate = 0.3),
    layer_dense(units = 1)
  ))
  
  # Compile
  model$compile(
    optimizer = optimizer_adam(learning_rate = 0.0001),
    loss = "mse",
    metrics = list("mae")
  )
  
  # Train with optimized parameters
  history <- model$fit(
    x = train_seq$x,
    y = train_seq$y,
    validation_split = 0.2,
    epochs = 300L,
    batch_size = 32L,
    callbacks = list(
      callback_early_stopping(
        monitor = "val_loss",
        patience = 30L,
        restore_best_weights = TRUE
      ),
      callback_reduce_lr_on_plateau(
        monitor = "val_loss",
        factor = 0.2,
        patience = 15L,
        min_lr = 0.000001
      )
    ),
    verbose = 1L
  )
  
  # Predictions and metrics
  train_pred <- model$predict(train_seq$x)
  test_pred <- model$predict(test_seq$x)
  
  train_pred <- as.vector(train_pred)
  test_pred <- as.vector(test_pred)
  train_y <- as.vector(train_seq$y)
  test_y <- as.vector(test_seq$y)
  
  # Calculate metrics
  train_rmse <- sqrt(mean((train_y - train_pred)^2))
  train_mae <- mean(abs(train_y - train_pred))
  test_rmse <- sqrt(mean((test_y - test_pred)^2))
  test_mae <- mean(abs(test_y - test_pred))
  
  # Calculate directional changes
  train_actual_direction <- sign(diff(train_y))
  train_pred_direction <- sign(diff(train_pred))
  test_actual_direction <- sign(diff(test_y))
  test_pred_direction <- sign(diff(test_pred))
  
  # Convert directions to binary (1 for up, 0 for down)
  train_actual_binary <- as.numeric(train_actual_direction > 0)
  train_pred_probs <- (train_pred_direction + 1) / 2  # Scale to [0,1]
  test_actual_binary <- as.numeric(test_actual_direction > 0)
  test_pred_probs <- (test_pred_direction + 1) / 2    # Scale to [0,1]
  
  # Calculate ROC curves
  train_roc <- roc(train_actual_binary, train_pred_probs)
  test_roc <- roc(test_actual_binary, test_pred_probs)
  
  # Calculate traditional direction accuracy
  train_direction_accuracy <- mean(train_actual_direction == train_pred_direction, na.rm = TRUE)
  test_direction_accuracy <- mean(test_actual_direction == test_pred_direction, na.rm = TRUE)
  
  list(
    model = model,
    history = history,
    predictions = list(
      train = train_pred,
      test = test_pred
    ),
    metrics = list(
      train = list(
        rmse = train_rmse,
        mae = train_mae,
        direction_accuracy = train_direction_accuracy,
        roc = train_roc,
        auc = as.numeric(auc(train_roc))
      ),
      test = list(
        rmse = test_rmse,
        mae = test_mae,
        direction_accuracy = test_direction_accuracy,
        roc = test_roc,
        auc = as.numeric(auc(test_roc))
      )
    )
  )
}

# Multiple runs function
run_multiple_lstms <- function(prepared_data, n_runs = 5) {
  results_list <- list()
  roc_curves <- list(
    train = list(),
    test = list()
  )
  
  for(i in 1:n_runs) {
    cat(sprintf("\nRunning model iteration %d of %d...\n", i, n_runs))
    
    model_results <- build_lstm_model(prepared_data)
    results_list[[i]] <- model_results$metrics$test
    
    # Store ROC curves
    roc_curves$train[[i]] <- model_results$metrics$train$roc
    roc_curves$test[[i]] <- model_results$metrics$test$roc
    
    cat(sprintf("Iteration %d Results:\n", i))
    cat(sprintf("RMSE: %.6f\n", results_list[[i]]$rmse))
    cat(sprintf("MAE: %.6f\n", results_list[[i]]$mae))
    cat(sprintf("Direction Accuracy: %.4f%%\n", results_list[[i]]$direction_accuracy * 100))
    cat(sprintf("AUC: %.4f\n", results_list[[i]]$auc))
  }
  
  # Calculate averages and standard deviations
  avg_metrics <- list(
    rmse = mean(sapply(results_list, function(x) x$rmse)),
    mae = mean(sapply(results_list, function(x) x$mae)),
    direction_accuracy = mean(sapply(results_list, function(x) x$direction_accuracy)),
    auc = mean(sapply(results_list, function(x) x$auc))
  )
  
  sd_metrics <- list(
    rmse = sd(sapply(results_list, function(x) x$rmse)),
    mae = sd(sapply(results_list, function(x) x$mae)),
    direction_accuracy = sd(sapply(results_list, function(x) x$direction_accuracy)),
    auc = sd(sapply(results_list, function(x) x$auc))
  )
  
  # Calculate average ROC curves
  avg_roc <- list(
    train = mean_roc_curve(roc_curves$train),
    test = mean_roc_curve(roc_curves$test)
  )
  
  cat("\n=== Summary Statistics ===\n")
  cat(sprintf("Average RMSE: %.6f (±%.6f)\n", avg_metrics$rmse, sd_metrics$rmse))
  cat(sprintf("Average MAE: %.6f (±%.6f)\n", avg_metrics$mae, sd_metrics$mae))
  cat(sprintf("Average Direction Accuracy: %.4f%% (±%.4f%%)\n", 
              avg_metrics$direction_accuracy * 100, 
              sd_metrics$direction_accuracy * 100))
  cat(sprintf("Average AUC: %.4f (±%.4f)\n",
              avg_metrics$auc,
              sd_metrics$auc))
  
  list(
    individual_results = results_list,
    average_metrics = avg_metrics,
    sd_metrics = sd_metrics,
    roc_curves = roc_curves,
    average_roc = avg_roc
  )
}

# Helper function to calculate mean ROC curve
mean_roc_curve <- function(roc_curves) {
  # Get a common set of specificity points
  spec_grid <- seq(0, 1, length.out = 100)
  
  # Interpolate sensitivities for each curve
  sens_mat <- sapply(roc_curves, function(roc) {
    coords(roc, spec_grid, "specificity", "sensitivity")$sensitivity
  })
  
  # Calculate mean sensitivity at each specificity point
  mean_sens <- rowMeans(sens_mat)
  
  # Create a data frame with the average ROC curve coordinates
  data.frame(
    specificity = spec_grid,
    sensitivity = mean_sens
  )
}

# After running your model:
prepared_data <- prepare_nn_data(jpm_train, jpm_test)
results <- run_multiple_lstms(prepared_data, n_runs = 3)

# Save ROC curve data
roc_jpm_nn <- data.frame(
    FPR = 1 - results$average_roc$test$specificity,  # False Positive Rate
    TPR = results$average_roc$test$sensitivity,      # True Positive Rate
    Model = "Neural Network"  # Adding model name for easier plotting later
)

# Save the AUC score separately if needed
auc_jpm_nn <- results$average_metrics$auc


```


NN FOR AAPLE:
```{r}
library(keras)
library(tidyverse)
library(scales)
library(TTR)
library(pROC)  # Added for ROC curve analysis

# Set seeds for reproducibility 
set.seed(201)
tensorflow::set_random_seed(201)

# Robust scaling function
robust_scale <- function(x) {
  (x - median(x)) / (IQR(x) + 1e-8)
}

# Enhanced data preparation using RF features
prepare_nn_data <- function(train_data, test_data) {
  # Create features consistently with Random Forest implementation
  create_rf_features <- function(data) {
    data %>%
      arrange(date) %>%
      mutate(
        # Target (5-day forward volatility)
        target_vol = rollapply(RET, width=5, FUN=sd, fill=NA, align="left") * sqrt(252),
        
        # Base return features
        ret_lag1 = lag(RET, 1),
        ret_lag2 = lag(RET, 2),
        ret_lag3 = lag(RET, 3),
        
        # Volatility features
        roll_vol_5d = rollapply(RET, width=5, FUN=sd, fill=NA, align="right") * sqrt(252),
        roll_vol_10d = rollapply(RET, width=10, FUN=sd, fill=NA, align="right") * sqrt(252),
        roll_vol_22d = rollapply(RET, width=22, FUN=sd, fill=NA, align="right") * sqrt(252),
        
        # Volatility momentum
        vol_momentum_5d = roll_vol_5d - lag(roll_vol_5d, 5),
        vol_momentum_ratio = vol_momentum_5d / roll_vol_5d,
        
        # Return patterns
        abs_ret = abs(RET),
        avg_abs_ret_5d = rollapply(abs(RET), width=5, FUN=mean, fill=NA, align="right"),
        
        # Market features
        mkt_vol_5d = rollapply(sprtrn, width=5, FUN=sd, fill=NA, align="right") * sqrt(252),
        mkt_vol_10d = rollapply(sprtrn, width=10, FUN=sd, fill=NA, align="right") * sqrt(252),
        
        # Additional features from RF
        market_rel_vol = roll_vol_5d / mkt_vol_5d,
        vol_trend_3d = rollapply(roll_vol_5d, 3, 
                                function(x) (x[3] - x[1])/x[1], 
                                fill=NA, align="right"),
        vol_ratio_5_22 = roll_vol_5d / roll_vol_22d,
        range_5d = rollapply(RET, width=5, 
                            FUN=function(x) max(x) - min(x), 
                            fill=NA, align="right"),
        
        # Feature interactions from RF
        vol_range_ratio = range_5d / roll_vol_5d,
        momentum_range_interact = vol_momentum_5d * range_5d,
        vol_market_interact = roll_vol_5d * mkt_vol_5d,
        range_momentum_ratio = range_5d / (vol_momentum_5d + 1e-8)
    )
  }
  
  # Create features for both datasets
  nn_train <- create_rf_features(train_data)
  nn_test <- create_rf_features(test_data)
  
  # Define features for normalization (all features from RF)
  feature_cols <- c(
    "ret_lag1", "ret_lag2", "ret_lag3",
    "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
    "vol_momentum_5d", "vol_momentum_ratio",
    "abs_ret", "avg_abs_ret_5d",
    "mkt_vol_5d", "mkt_vol_10d",
    "market_rel_vol", "vol_trend_3d",
    "vol_ratio_5_22", "range_5d",
    "vol_range_ratio", "momentum_range_interact",
    "vol_market_interact", "range_momentum_ratio"
  )
  
  # Calculate robust scaling parameters from training data
  scale_params <- lapply(feature_cols, function(col) {
    list(
      median = median(nn_train[[col]], na.rm = TRUE),
      iqr = IQR(nn_train[[col]], na.rm = TRUE)
    )
  })
  names(scale_params) <- feature_cols
  
  # Apply robust scaling
  for(col in feature_cols) {
    nn_train[[paste0(col, "_norm")]] <- (nn_train[[col]] - scale_params[[col]]$median) / 
                                       (scale_params[[col]]$iqr + 1e-8)
    nn_test[[paste0(col, "_norm")]] <- (nn_test[[col]] - scale_params[[col]]$median) / 
                                      (scale_params[[col]]$iqr + 1e-8)
  }
  
  # Remove NA values
  nn_train <- nn_train %>% na.omit()
  nn_test <- nn_test %>% na.omit()
  
  list(
    train = nn_train,
    test = nn_test,
    scale_params = scale_params,
    feature_cols = paste0(feature_cols, "_norm")
  )
}

# Create sequences
create_sequences <- function(data, sequence_length, feature_cols) {
  n <- nrow(data) - sequence_length
  x <- array(0, dim = c(n, sequence_length, length(feature_cols)))
  y <- array(0, dim = c(n))
  
  for(i in 1:n) {
    x[i,,] <- as.matrix(data[i:(i+sequence_length-1), feature_cols])
    y[i] <- data$target_vol[i+sequence_length]
  }
  
  list(x = x, y = y)
}

# Build and train LSTM model
build_lstm_model <- function(prepared_data, sequence_length = 30) {
  train_seq <- create_sequences(prepared_data$train, 
                              sequence_length, 
                              prepared_data$feature_cols)
  test_seq <- create_sequences(prepared_data$test, 
                             sequence_length, 
                             prepared_data$feature_cols)
  
  n_features <- length(prepared_data$feature_cols)
  
  # Define model with optimized architecture
  model <- keras_model_sequential(layers = list(
    # First LSTM block
    layer_lstm(units = 256,
              input_shape = c(sequence_length, n_features),
              return_sequences = TRUE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Second LSTM block
    layer_lstm(units = 128,
              return_sequences = TRUE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Third LSTM block
    layer_lstm(units = 64,
              return_sequences = FALSE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Dense layers
    layer_dense(units = 64, activation = "relu"),
    layer_dropout(rate = 0.3),
    layer_dense(units = 32, activation = "relu"),
    layer_dropout(rate = 0.3),
    layer_dense(units = 1)
  ))
  
  # Compile
  model$compile(
    optimizer = optimizer_adam(learning_rate = 0.0001),
    loss = "mse",
    metrics = list("mae")
  )
  
  # Train with optimized parameters
  history <- model$fit(
    x = train_seq$x,
    y = train_seq$y,
    validation_split = 0.2,
    epochs = 300L,
    batch_size = 32L,
    callbacks = list(
      callback_early_stopping(
        monitor = "val_loss",
        patience = 30L,
        restore_best_weights = TRUE
      ),
      callback_reduce_lr_on_plateau(
        monitor = "val_loss",
        factor = 0.2,
        patience = 15L,
        min_lr = 0.000001
      )
    ),
    verbose = 1L
  )
  
  # Predictions and metrics
  train_pred <- model$predict(train_seq$x)
  test_pred <- model$predict(test_seq$x)
  
  train_pred <- as.vector(train_pred)
  test_pred <- as.vector(test_pred)
  train_y <- as.vector(train_seq$y)
  test_y <- as.vector(test_seq$y)
  
  # Calculate metrics
  train_rmse <- sqrt(mean((train_y - train_pred)^2))
  train_mae <- mean(abs(train_y - train_pred))
  test_rmse <- sqrt(mean((test_y - test_pred)^2))
  test_mae <- mean(abs(test_y - test_pred))
  
  # Calculate directional changes
  train_actual_direction <- sign(diff(train_y))
  train_pred_direction <- sign(diff(train_pred))
  test_actual_direction <- sign(diff(test_y))
  test_pred_direction <- sign(diff(test_pred))
  
  # Convert directions to binary (1 for up, 0 for down)
  train_actual_binary <- as.numeric(train_actual_direction > 0)
  train_pred_probs <- (train_pred_direction + 1) / 2  # Scale to [0,1]
  test_actual_binary <- as.numeric(test_actual_direction > 0)
  test_pred_probs <- (test_pred_direction + 1) / 2    # Scale to [0,1]
  
  # Calculate ROC curves
  train_roc <- roc(train_actual_binary, train_pred_probs)
  test_roc <- roc(test_actual_binary, test_pred_probs)
  
  # Calculate traditional direction accuracy
  train_direction_accuracy <- mean(train_actual_direction == train_pred_direction, na.rm = TRUE)
  test_direction_accuracy <- mean(test_actual_direction == test_pred_direction, na.rm = TRUE)
  
  list(
    model = model,
    history = history,
    predictions = list(
      train = train_pred,
      test = test_pred
    ),
    metrics = list(
      train = list(
        rmse = train_rmse,
        mae = train_mae,
        direction_accuracy = train_direction_accuracy,
        roc = train_roc,
        auc = as.numeric(auc(train_roc))
      ),
      test = list(
        rmse = test_rmse,
        mae = test_mae,
        direction_accuracy = test_direction_accuracy,
        roc = test_roc,
        auc = as.numeric(auc(test_roc))
      )
    )
  )
}

# Multiple runs function
run_multiple_lstms <- function(prepared_data, n_runs = 5) {
  results_list <- list()
  roc_curves <- list(
    train = list(),
    test = list()
  )
  
  for(i in 1:n_runs) {
    cat(sprintf("\nRunning model iteration %d of %d...\n", i, n_runs))
    
    model_results <- build_lstm_model(prepared_data)
    results_list[[i]] <- model_results$metrics$test
    
    # Store ROC curves
    roc_curves$train[[i]] <- model_results$metrics$train$roc
    roc_curves$test[[i]] <- model_results$metrics$test$roc
    
    cat(sprintf("Iteration %d Results:\n", i))
    cat(sprintf("RMSE: %.6f\n", results_list[[i]]$rmse))
    cat(sprintf("MAE: %.6f\n", results_list[[i]]$mae))
    cat(sprintf("Direction Accuracy: %.4f%%\n", results_list[[i]]$direction_accuracy * 100))
    cat(sprintf("AUC: %.4f\n", results_list[[i]]$auc))
  }
  
  # Calculate averages and standard deviations
  avg_metrics <- list(
    rmse = mean(sapply(results_list, function(x) x$rmse)),
    mae = mean(sapply(results_list, function(x) x$mae)),
    direction_accuracy = mean(sapply(results_list, function(x) x$direction_accuracy)),
    auc = mean(sapply(results_list, function(x) x$auc))
  )
  
  sd_metrics <- list(
    rmse = sd(sapply(results_list, function(x) x$rmse)),
    mae = sd(sapply(results_list, function(x) x$mae)),
    direction_accuracy = sd(sapply(results_list, function(x) x$direction_accuracy)),
    auc = sd(sapply(results_list, function(x) x$auc))
  )
  
  # Calculate average ROC curves
  avg_roc <- list(
    train = mean_roc_curve(roc_curves$train),
    test = mean_roc_curve(roc_curves$test)
  )
  
  cat("\n=== Summary Statistics ===\n")
  cat(sprintf("Average RMSE: %.6f (±%.6f)\n", avg_metrics$rmse, sd_metrics$rmse))
  cat(sprintf("Average MAE: %.6f (±%.6f)\n", avg_metrics$mae, sd_metrics$mae))
  cat(sprintf("Average Direction Accuracy: %.4f%% (±%.4f%%)\n", 
              avg_metrics$direction_accuracy * 100, 
              sd_metrics$direction_accuracy * 100))
  cat(sprintf("Average AUC: %.4f (±%.4f)\n",
              avg_metrics$auc,
              sd_metrics$auc))
  
  list(
    individual_results = results_list,
    average_metrics = avg_metrics,
    sd_metrics = sd_metrics,
    roc_curves = roc_curves,
    average_roc = avg_roc
  )
}

# Helper function to calculate mean ROC curve
mean_roc_curve <- function(roc_curves) {
  # Get a common set of specificity points
  spec_grid <- seq(0, 1, length.out = 100)
  
  # Interpolate sensitivities for each curve
  sens_mat <- sapply(roc_curves, function(roc) {
    coords(roc, spec_grid, "specificity", "sensitivity")$sensitivity
  })
  
  # Calculate mean sensitivity at each specificity point
  mean_sens <- rowMeans(sens_mat)
  
  # Create a data frame with the average ROC curve coordinates
  data.frame(
    specificity = spec_grid,
    sensitivity = mean_sens
  )
}

# After running your model:
prepared_data <- prepare_nn_data(apple_train, apple_test)
results <- run_multiple_lstms(prepared_data, n_runs = 3)

# Save ROC curve data
roc_apple_nn <- data.frame(
    FPR = 1 - results$average_roc$test$specificity,  # False Positive Rate
    TPR = results$average_roc$test$sensitivity,      # True Positive Rate
    Model = "Neural Network"  # Adding model name for easier plotting later
)

# Save the AUC score separately if needed
auc_apple_nn <- results$average_metrics$auc


```

NN FOR TESLA:
```{r}
library(keras)
library(tidyverse)
library(scales)
library(TTR)
library(pROC)  # Added for ROC curve analysis

# Set seeds for reproducibility 
set.seed(201)
tensorflow::set_random_seed(201)

# Robust scaling function
robust_scale <- function(x) {
  (x - median(x)) / (IQR(x) + 1e-8)
}

# Enhanced data preparation using RF features
prepare_nn_data <- function(train_data, test_data) {
  # Create features consistently with Random Forest implementation
  create_rf_features <- function(data) {
    data %>%
      arrange(date) %>%
      mutate(
        # Target (5-day forward volatility)
        target_vol = rollapply(RET, width=5, FUN=sd, fill=NA, align="left") * sqrt(252),
        
        # Base return features
        ret_lag1 = lag(RET, 1),
        ret_lag2 = lag(RET, 2),
        ret_lag3 = lag(RET, 3),
        
        # Volatility features
        roll_vol_5d = rollapply(RET, width=5, FUN=sd, fill=NA, align="right") * sqrt(252),
        roll_vol_10d = rollapply(RET, width=10, FUN=sd, fill=NA, align="right") * sqrt(252),
        roll_vol_22d = rollapply(RET, width=22, FUN=sd, fill=NA, align="right") * sqrt(252),
        
        # Volatility momentum
        vol_momentum_5d = roll_vol_5d - lag(roll_vol_5d, 5),
        vol_momentum_ratio = vol_momentum_5d / roll_vol_5d,
        
        # Return patterns
        abs_ret = abs(RET),
        avg_abs_ret_5d = rollapply(abs(RET), width=5, FUN=mean, fill=NA, align="right"),
        
        # Market features
        mkt_vol_5d = rollapply(sprtrn, width=5, FUN=sd, fill=NA, align="right") * sqrt(252),
        mkt_vol_10d = rollapply(sprtrn, width=10, FUN=sd, fill=NA, align="right") * sqrt(252),
        
        # Additional features from RF
        market_rel_vol = roll_vol_5d / mkt_vol_5d,
        vol_trend_3d = rollapply(roll_vol_5d, 3, 
                                function(x) (x[3] - x[1])/x[1], 
                                fill=NA, align="right"),
        vol_ratio_5_22 = roll_vol_5d / roll_vol_22d,
        range_5d = rollapply(RET, width=5, 
                            FUN=function(x) max(x) - min(x), 
                            fill=NA, align="right"),
        
        # Feature interactions from RF
        vol_range_ratio = range_5d / roll_vol_5d,
        momentum_range_interact = vol_momentum_5d * range_5d,
        vol_market_interact = roll_vol_5d * mkt_vol_5d,
        range_momentum_ratio = range_5d / (vol_momentum_5d + 1e-8)
    )
  }
  
  # Create features for both datasets
  nn_train <- create_rf_features(train_data)
  nn_test <- create_rf_features(test_data)
  
  # Define features for normalization (all features from RF)
  feature_cols <- c(
    "ret_lag1", "ret_lag2", "ret_lag3",
    "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
    "vol_momentum_5d", "vol_momentum_ratio",
    "abs_ret", "avg_abs_ret_5d",
    "mkt_vol_5d", "mkt_vol_10d",
    "market_rel_vol", "vol_trend_3d",
    "vol_ratio_5_22", "range_5d",
    "vol_range_ratio", "momentum_range_interact",
    "vol_market_interact", "range_momentum_ratio"
  )
  
  # Calculate robust scaling parameters from training data
  scale_params <- lapply(feature_cols, function(col) {
    list(
      median = median(nn_train[[col]], na.rm = TRUE),
      iqr = IQR(nn_train[[col]], na.rm = TRUE)
    )
  })
  names(scale_params) <- feature_cols
  
  # Apply robust scaling
  for(col in feature_cols) {
    nn_train[[paste0(col, "_norm")]] <- (nn_train[[col]] - scale_params[[col]]$median) / 
                                       (scale_params[[col]]$iqr + 1e-8)
    nn_test[[paste0(col, "_norm")]] <- (nn_test[[col]] - scale_params[[col]]$median) / 
                                      (scale_params[[col]]$iqr + 1e-8)
  }
  
  # Remove NA values
  nn_train <- nn_train %>% na.omit()
  nn_test <- nn_test %>% na.omit()
  
  list(
    train = nn_train,
    test = nn_test,
    scale_params = scale_params,
    feature_cols = paste0(feature_cols, "_norm")
  )
}

# Create sequences
create_sequences <- function(data, sequence_length, feature_cols) {
  n <- nrow(data) - sequence_length
  x <- array(0, dim = c(n, sequence_length, length(feature_cols)))
  y <- array(0, dim = c(n))
  
  for(i in 1:n) {
    x[i,,] <- as.matrix(data[i:(i+sequence_length-1), feature_cols])
    y[i] <- data$target_vol[i+sequence_length]
  }
  
  list(x = x, y = y)
}

# Build and train LSTM model
build_lstm_model <- function(prepared_data, sequence_length = 30) {
  train_seq <- create_sequences(prepared_data$train, 
                              sequence_length, 
                              prepared_data$feature_cols)
  test_seq <- create_sequences(prepared_data$test, 
                             sequence_length, 
                             prepared_data$feature_cols)
  
  n_features <- length(prepared_data$feature_cols)
  
  # Define model with optimized architecture
  model <- keras_model_sequential(layers = list(
    # First LSTM block
    layer_lstm(units = 256,
              input_shape = c(sequence_length, n_features),
              return_sequences = TRUE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Second LSTM block
    layer_lstm(units = 128,
              return_sequences = TRUE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Third LSTM block
    layer_lstm(units = 64,
              return_sequences = FALSE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Dense layers
    layer_dense(units = 64, activation = "relu"),
    layer_dropout(rate = 0.3),
    layer_dense(units = 32, activation = "relu"),
    layer_dropout(rate = 0.3),
    layer_dense(units = 1)
  ))
  
  # Compile
  model$compile(
    optimizer = optimizer_adam(learning_rate = 0.0001),
    loss = "mse",
    metrics = list("mae")
  )
  
  # Train with optimized parameters
  history <- model$fit(
    x = train_seq$x,
    y = train_seq$y,
    validation_split = 0.2,
    epochs = 300L,
    batch_size = 32L,
    callbacks = list(
      callback_early_stopping(
        monitor = "val_loss",
        patience = 30L,
        restore_best_weights = TRUE
      ),
      callback_reduce_lr_on_plateau(
        monitor = "val_loss",
        factor = 0.2,
        patience = 15L,
        min_lr = 0.000001
      )
    ),
    verbose = 1L
  )
  
  # Predictions and metrics
  train_pred <- model$predict(train_seq$x)
  test_pred <- model$predict(test_seq$x)
  
  train_pred <- as.vector(train_pred)
  test_pred <- as.vector(test_pred)
  train_y <- as.vector(train_seq$y)
  test_y <- as.vector(test_seq$y)
  
  # Calculate metrics
  train_rmse <- sqrt(mean((train_y - train_pred)^2))
  train_mae <- mean(abs(train_y - train_pred))
  test_rmse <- sqrt(mean((test_y - test_pred)^2))
  test_mae <- mean(abs(test_y - test_pred))
  
  # Calculate directional changes
  train_actual_direction <- sign(diff(train_y))
  train_pred_direction <- sign(diff(train_pred))
  test_actual_direction <- sign(diff(test_y))
  test_pred_direction <- sign(diff(test_pred))
  
  # Convert directions to binary (1 for up, 0 for down)
  train_actual_binary <- as.numeric(train_actual_direction > 0)
  train_pred_probs <- (train_pred_direction + 1) / 2  # Scale to [0,1]
  test_actual_binary <- as.numeric(test_actual_direction > 0)
  test_pred_probs <- (test_pred_direction + 1) / 2    # Scale to [0,1]
  
  # Calculate ROC curves
  train_roc <- roc(train_actual_binary, train_pred_probs)
  test_roc <- roc(test_actual_binary, test_pred_probs)
  
  # Calculate traditional direction accuracy
  train_direction_accuracy <- mean(train_actual_direction == train_pred_direction, na.rm = TRUE)
  test_direction_accuracy <- mean(test_actual_direction == test_pred_direction, na.rm = TRUE)
  
  list(
    model = model,
    history = history,
    predictions = list(
      train = train_pred,
      test = test_pred
    ),
    metrics = list(
      train = list(
        rmse = train_rmse,
        mae = train_mae,
        direction_accuracy = train_direction_accuracy,
        roc = train_roc,
        auc = as.numeric(auc(train_roc))
      ),
      test = list(
        rmse = test_rmse,
        mae = test_mae,
        direction_accuracy = test_direction_accuracy,
        roc = test_roc,
        auc = as.numeric(auc(test_roc))
      )
    )
  )
}

# Multiple runs function
run_multiple_lstms <- function(prepared_data, n_runs = 5) {
  results_list <- list()
  roc_curves <- list(
    train = list(),
    test = list()
  )
  
  for(i in 1:n_runs) {
    cat(sprintf("\nRunning model iteration %d of %d...\n", i, n_runs))
    
    model_results <- build_lstm_model(prepared_data)
    results_list[[i]] <- model_results$metrics$test
    
    # Store ROC curves
    roc_curves$train[[i]] <- model_results$metrics$train$roc
    roc_curves$test[[i]] <- model_results$metrics$test$roc
    
    cat(sprintf("Iteration %d Results:\n", i))
    cat(sprintf("RMSE: %.6f\n", results_list[[i]]$rmse))
    cat(sprintf("MAE: %.6f\n", results_list[[i]]$mae))
    cat(sprintf("Direction Accuracy: %.4f%%\n", results_list[[i]]$direction_accuracy * 100))
    cat(sprintf("AUC: %.4f\n", results_list[[i]]$auc))
  }
  
  # Calculate averages and standard deviations
  avg_metrics <- list(
    rmse = mean(sapply(results_list, function(x) x$rmse)),
    mae = mean(sapply(results_list, function(x) x$mae)),
    direction_accuracy = mean(sapply(results_list, function(x) x$direction_accuracy)),
    auc = mean(sapply(results_list, function(x) x$auc))
  )
  
  sd_metrics <- list(
    rmse = sd(sapply(results_list, function(x) x$rmse)),
    mae = sd(sapply(results_list, function(x) x$mae)),
    direction_accuracy = sd(sapply(results_list, function(x) x$direction_accuracy)),
    auc = sd(sapply(results_list, function(x) x$auc))
  )
  
  # Calculate average ROC curves
  avg_roc <- list(
    train = mean_roc_curve(roc_curves$train),
    test = mean_roc_curve(roc_curves$test)
  )
  
  cat("\n=== Summary Statistics ===\n")
  cat(sprintf("Average RMSE: %.6f (±%.6f)\n", avg_metrics$rmse, sd_metrics$rmse))
  cat(sprintf("Average MAE: %.6f (±%.6f)\n", avg_metrics$mae, sd_metrics$mae))
  cat(sprintf("Average Direction Accuracy: %.4f%% (±%.4f%%)\n", 
              avg_metrics$direction_accuracy * 100, 
              sd_metrics$direction_accuracy * 100))
  cat(sprintf("Average AUC: %.4f (±%.4f)\n",
              avg_metrics$auc,
              sd_metrics$auc))
  
  list(
    individual_results = results_list,
    average_metrics = avg_metrics,
    sd_metrics = sd_metrics,
    roc_curves = roc_curves,
    average_roc = avg_roc
  )
}

# Helper function to calculate mean ROC curve
mean_roc_curve <- function(roc_curves) {
  # Get a common set of specificity points
  spec_grid <- seq(0, 1, length.out = 100)
  
  # Interpolate sensitivities for each curve
  sens_mat <- sapply(roc_curves, function(roc) {
    coords(roc, spec_grid, "specificity", "sensitivity")$sensitivity
  })
  
  # Calculate mean sensitivity at each specificity point
  mean_sens <- rowMeans(sens_mat)
  
  # Create a data frame with the average ROC curve coordinates
  data.frame(
    specificity = spec_grid,
    sensitivity = mean_sens
  )
}

# After running your model:
prepared_data <- prepare_nn_data(tesla_train, tesla_test)
results <- run_multiple_lstms(prepared_data, n_runs = 3)

# Save ROC curve data
roc_tesla_nn <- data.frame(
    FPR = 1 - results$average_roc$test$specificity,  # False Positive Rate
    TPR = results$average_roc$test$sensitivity,      # True Positive Rate
    Model = "Neural Network"  # Adding model name for easier plotting later
)

# Save the AUC score separately if needed
auc_tesla_nn <- results$average_metrics$auc


```
Tried using more data (from 2000) for JPM as i thought this could be why the NN was performing worse, but this actually further reduced the prediction accuracy and increased the MAE/RMSE. Potentially due to the amount of volatile periods (dotcom bubble, financial crisis, covid, flash sales, etc.)



***Visualisation of Neural Network Code:***


```{r}
# Load required libraries
library(keras)
library(tidyverse)
library(scales)
library(TTR)

# Set seeds for reproducibility 
set.seed(201)
tensorflow::set_random_seed(201)

# Robust scaling function
robust_scale <- function(x) {
  (x - median(x)) / (IQR(x) + 1e-8)
}

# Enhanced data preparation using RF features
prepare_nn_data <- function(train_data, test_data) {
  # Create features consistently with Random Forest implementation
  create_rf_features <- function(data) {
    data %>%
      arrange(date) %>%
      mutate(
        # Target (5-day forward volatility)
        target_vol = rollapply(RET, width=5, FUN=sd, fill=NA, align="left") * sqrt(252),
        
        # Base return features
        ret_lag1 = lag(RET, 1),
        ret_lag2 = lag(RET, 2),
        ret_lag3 = lag(RET, 3),
        
        # Volatility features
        roll_vol_5d = rollapply(RET, width=5, FUN=sd, fill=NA, align="right") * sqrt(252),
        roll_vol_10d = rollapply(RET, width=10, FUN=sd, fill=NA, align="right") * sqrt(252),
        roll_vol_22d = rollapply(RET, width=22, FUN=sd, fill=NA, align="right") * sqrt(252),
        
        # Volatility momentum
        vol_momentum_5d = roll_vol_5d - lag(roll_vol_5d, 5),
        vol_momentum_ratio = vol_momentum_5d / roll_vol_5d,
        
        # Return patterns
        abs_ret = abs(RET),
        avg_abs_ret_5d = rollapply(abs(RET), width=5, FUN=mean, fill=NA, align="right"),
        
        # Market features
        mkt_vol_5d = rollapply(sprtrn, width=5, FUN=sd, fill=NA, align="right") * sqrt(252),
        mkt_vol_10d = rollapply(sprtrn, width=10, FUN=sd, fill=NA, align="right") * sqrt(252),
        
        # Additional features from RF
        market_rel_vol = roll_vol_5d / mkt_vol_5d,
        vol_trend_3d = rollapply(roll_vol_5d, 3, 
                                function(x) (x[3] - x[1])/x[1], 
                                fill=NA, align="right"),
        vol_ratio_5_22 = roll_vol_5d / roll_vol_22d,
        range_5d = rollapply(RET, width=5, 
                            FUN=function(x) max(x) - min(x), 
                            fill=NA, align="right"),
        
        # Feature interactions from RF
        vol_range_ratio = range_5d / roll_vol_5d,
        momentum_range_interact = vol_momentum_5d * range_5d,
        vol_market_interact = roll_vol_5d * mkt_vol_5d,
        range_momentum_ratio = range_5d / (vol_momentum_5d + 1e-8)
    )
  }
  
  # Create features for both datasets
  nn_train <- create_rf_features(train_data)
  nn_test <- create_rf_features(test_data)
  
  # Define features for normalization
  feature_cols <- c(
    "ret_lag1", "ret_lag2", "ret_lag3",
    "roll_vol_5d", "roll_vol_10d", "roll_vol_22d",
    "vol_momentum_5d", "vol_momentum_ratio",
    "abs_ret", "avg_abs_ret_5d",
    "mkt_vol_5d", "mkt_vol_10d",
    "market_rel_vol", "vol_trend_3d",
    "vol_ratio_5_22", "range_5d",
    "vol_range_ratio", "momentum_range_interact",
    "vol_market_interact", "range_momentum_ratio"
  )
  
  # Calculate robust scaling parameters from training data
  scale_params <- lapply(feature_cols, function(col) {
    list(
      median = median(nn_train[[col]], na.rm = TRUE),
      iqr = IQR(nn_train[[col]], na.rm = TRUE)
    )
  })
  names(scale_params) <- feature_cols
  
  # Apply robust scaling
  for(col in feature_cols) {
    nn_train[[paste0(col, "_norm")]] <- (nn_train[[col]] - scale_params[[col]]$median) / 
                                       (scale_params[[col]]$iqr + 1e-8)
    nn_test[[paste0(col, "_norm")]] <- (nn_test[[col]] - scale_params[[col]]$median) / 
                                      (scale_params[[col]]$iqr + 1e-8)
  }
  
  # Remove NA values
  nn_train <- nn_train %>% na.omit()
  nn_test <- nn_test %>% na.omit()
  
  list(
    train = nn_train,
    test = nn_test,
    scale_params = scale_params,
    feature_cols = paste0(feature_cols, "_norm")
  )
}

# Create sequences
create_sequences <- function(data, sequence_length, feature_cols) {
  n <- nrow(data) - sequence_length
  x <- array(0, dim = c(n, sequence_length, length(feature_cols)))
  y <- array(0, dim = c(n))
  
  for(i in 1:n) {
    x[i,,] <- as.matrix(data[i:(i+sequence_length-1), feature_cols])
    y[i] <- data$target_vol[i+sequence_length]
  }
  
  list(x = x, y = y)
}

# Enhanced visualization function that creates a detailed text representation
visualize_network_architecture <- function(model) {
  # Create a detailed visualization using DiagrammeR
  library(DiagrammeR)
  
  # Extract layer information
  layers <- model$layers
  nodes <- data.frame(
    id = seq_along(layers),
    label = sapply(layers, function(x) class(x)[1]),
    shape = "rectangle",
    style = "filled",
    fillcolor = "lightblue",
    stringsAsFactors = FALSE
  )
  
  # Create edges between layers
  edges <- data.frame(
    from = 1:(length(layers)-1),
    to = 2:length(layers),
    stringsAsFactors = FALSE
  )
  
  # Create the graph
  graph <- create_graph(nodes_df = nodes, edges_df = edges) %>%
    add_global_graph_attrs(
      attr = "rankdir",
      value = "TB",
      attr_type = "graph"
    )
  
  # Print text summary
  cat("\nModel Architecture Summary:\n")
  cat("==========================\n")
  model %>% summary()
  
  # Render the graph
  render_graph(graph)
}

# Build and train LSTM model with visualization
build_lstm_model <- function(prepared_data, sequence_length = 30) {
  train_seq <- create_sequences(prepared_data$train, 
                              sequence_length, 
                              prepared_data$feature_cols)
  test_seq <- create_sequences(prepared_data$test, 
                             sequence_length, 
                             prepared_data$feature_cols)
  
  n_features <- length(prepared_data$feature_cols)
  
  # Define model with optimized architecture
  model <- keras_model_sequential(layers = list(
    # First LSTM block
    layer_lstm(units = 256,
              input_shape = c(sequence_length, n_features),
              return_sequences = TRUE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Second LSTM block
    layer_lstm(units = 128,
              return_sequences = TRUE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Third LSTM block
    layer_lstm(units = 64,
              return_sequences = FALSE),
    layer_dropout(rate = 0.4),
    layer_batch_normalization(),
    
    # Dense layers
    layer_dense(units = 64, activation = "relu"),
    layer_dropout(rate = 0.3),
    layer_dense(units = 32, activation = "relu"),
    layer_dropout(rate = 0.3),
    layer_dense(units = 1)
  ))
  
  # Compile
  model$compile(
    optimizer = optimizer_adam(learning_rate = 0.0001),
    loss = "mse",
    metrics = list("mae")
  )
  
  # Visualize the model architecture before training
  cat("\nInitial Model Architecture:\n")
  visualize_network_architecture(model)
  
  # Train with optimized parameters
  history <- model$fit(
    x = train_seq$x,
    y = train_seq$y,
    validation_split = 0.2,
    epochs = 300L,
    batch_size = 32L,
    callbacks = list(
      callback_early_stopping(
        monitor = "val_loss",
        patience = 30L,
        restore_best_weights = TRUE
      ),
      callback_reduce_lr_on_plateau(
        monitor = "val_loss",
        factor = 0.2,
        patience = 15L,
        min_lr = 0.000001
      )
    ),
    verbose = 1L
  )
  
  # Predictions and metrics
  train_pred <- model$predict(train_seq$x)
  test_pred <- model$predict(test_seq$x)
  
  train_pred <- as.vector(train_pred)
  test_pred <- as.vector(test_pred)
  train_y <- as.vector(train_seq$y)
  test_y <- as.vector(test_seq$y)
  
  # Calculate metrics
  train_rmse <- sqrt(mean((train_y - train_pred)^2))
  train_mae <- mean(abs(train_y - train_pred))
  test_rmse <- sqrt(mean((test_y - test_pred)^2))
  test_mae <- mean(abs(test_y - test_pred))
  
  train_direction_accuracy <- mean(sign(diff(train_pred)) == sign(diff(train_y)))
  test_direction_accuracy <- mean(sign(diff(test_pred)) == sign(diff(test_y)))
  
  list(
    model = model,
    history = history,
    predictions = list(
      train = train_pred,
      test = test_pred
    ),
    metrics = list(
      train = list(
        rmse = train_rmse,
        mae = train_mae,
        direction_accuracy = train_direction_accuracy
      ),
      test = list(
        rmse = test_rmse,
        mae = test_mae,
        direction_accuracy = test_direction_accuracy
      )
    )
  )
}

# Run a single LSTM model with visualization
run_single_lstm <- function(prepared_data) {
  cat("\nTraining LSTM model...\n")
  
  model_results <- build_lstm_model(prepared_data)
  
  # Print metrics
  cat("\n=== Model Performance Metrics ===\n")
  cat(sprintf("Test RMSE: %.6f\n", model_results$metrics$test$rmse))
  cat(sprintf("Test MAE: %.6f\n", model_results$metrics$test$mae))
  cat(sprintf("Test Direction Accuracy: %.4f%%\n", 
              model_results$metrics$test$direction_accuracy * 100))
  
  return(model_results)
}

# Usage
# First prepare the data
prepared_data <- prepare_nn_data(jpm_train, jpm_test)

# Then run the single model with visualization
results <- run_single_lstm(prepared_data)




### VISUALISATION BELOW OF NN



library(DiagrammeR)

visualize_network_architecture <- function(model) {
  # Create GraphViz DOT language string for better control
  dot_string <- 'digraph G {
    rankdir=LR;  // Left to right layout
    splines=ortho;  // Orthogonal lines
    nodesep=0.5;    // Node separation
    ranksep=1.5;    // Rank separation
    
    // Node styling
    node [shape=circle, style=filled, fillcolor=lightblue, fixedsize=true, width=0.6];
    
    // Input layer
    subgraph cluster_input {
      label="Input Layer\n(20 features)";
      style=filled;
      color=lightgrey;
      fillcolor=white;
      node [fillcolor=lightgreen];
      {rank=same; I1; I2; I3; I4; I5; Idots}
      I1 [label="1"];
      I2 [label="2"];
      I3 [label="3"];
      I4 [label="4"];
      I5 [label="5"];
      Idots [label="..."];
    }
    
    // LSTM layers
    subgraph cluster_lstm1 {
      label="LSTM 256";
      style=filled;
      color=lightgrey;
      fillcolor=white;
      node [fillcolor=lightblue];
      {rank=same; L1_1; L1_2; L1_3; L1_dots}
      L1_1 [label="1"];
      L1_2 [label="2"];
      L1_3 [label="3"];
      L1_dots [label="..."];
    }
    
    subgraph cluster_lstm2 {
      label="LSTM 128";
      style=filled;
      color=lightgrey;
      fillcolor=white;
      node [fillcolor=lightblue];
      {rank=same; L2_1; L2_2; L2_3; L2_dots}
      L2_1 [label="1"];
      L2_2 [label="2"];
      L2_3 [label="3"];
      L2_dots [label="..."];
    }
    
    subgraph cluster_lstm3 {
      label="LSTM 64";
      style=filled;
      color=lightgrey;
      fillcolor=white;
      node [fillcolor=lightblue];
      {rank=same; L3_1; L3_2; L3_3; L3_dots}
      L3_1 [label="1"];
      L3_2 [label="2"];
      L3_3 [label="3"];
      L3_dots [label="..."];
    }
    
    // Dense layers
    subgraph cluster_dense {
      label="Dense Layers";
      style=filled;
      color=lightgrey;
      fillcolor=white;
      node [fillcolor=lightsalmon];
      D1 [label="64"];
      D2 [label="32"];
      Output [label="1"];
    }
    
    // Connections between layers with reduced opacity
    edge [color="#00000040"];  // Semi-transparent black
    
    // Connect input to first LSTM
    {I1; I2; I3; I4; I5} -> {L1_1; L1_2; L1_3}
    
    // Connect LSTM layers
    {L1_1; L1_2; L1_3} -> {L2_1; L2_2; L2_3}
    {L2_1; L2_2; L2_3} -> {L3_1; L3_2; L3_3}
    
    // Connect to dense layers
    {L3_1; L3_2; L3_3} -> D1
    D1 -> D2 -> Output
    
    // Label
    label = "LSTM Neural Network Architecture";
    labelloc = "t";
    fontsize = 20;
  }'
  
  grViz(dot_string)
}

# Use it with your existing model
visualize_network_architecture(results$model)
```




***ROC CURVE PLOT***

```{r}
# First calculate the ROC curves for each model and stock
# Then interpolate them to common FPR points before averaging

# Create a sequence of common FPR points for interpolation
fpr_grid <- seq(0, 1, by = 0.01)

# Function to interpolate ROC curve to common points
interpolate_roc <- function(roc) {
  approx(x = 1 - roc$specificities, 
         y = roc$sensitivities, 
         xout = fpr_grid)$y
}

# Calculate interpolated curves for each model
# Decision Tree
dt_interp <- data.frame(
  fpr = fpr_grid,
  tpr = rowMeans(cbind(
    interpolate_roc(roc_apple_dt),
    interpolate_roc(roc_jpm_dt),
    interpolate_roc(roc_tesla_dt)
  ))
)

# Random Forest
rf_interp <- data.frame(
  fpr = fpr_grid,
  tpr = rowMeans(cbind(
    interpolate_roc(roc_apple_rf),
    interpolate_roc(roc_jpm_rf),
    interpolate_roc(roc_tesla_rf)
  ))
)

# Do the same for other models...
garch_interp <- data.frame(
  fpr = fpr_grid,
  tpr = rowMeans(cbind(
    interpolate_roc(roc_apple_garch),
    interpolate_roc(roc_jpm_garch),
    interpolate_roc(roc_tsla_garch)
  ))
)

gbm_interp <- data.frame(
  fpr = fpr_grid,
  tpr = rowMeans(cbind(
    interpolate_roc(roc_apple_gbm),
    interpolate_roc(roc_jpm_gbm),
    interpolate_roc(roc_tesla_gbm)
  ))
)

nn_interp <- data.frame(
  fpr = fpr_grid,
  tpr = rowMeans(cbind(
    interpolate_roc(roc_apple_nn2),
    interpolate_roc(roc_jpm_nn2),
    interpolate_roc(roc_tesla_nn2)
  ))
)

# Combine all interpolated curves
plot_data <- rbind(
  cbind(dt_interp, model = sprintf("Decision Tree\n(AUC = %.3f)", auc_dt)),
  cbind(garch_interp, model = sprintf("GARCH\n(AUC = %.3f)", auc_garch)),
  cbind(gbm_interp, model = sprintf("Gradient Boosting\n(AUC = %.3f)", auc_gbm)),
  cbind(nn_interp, model = sprintf("Neural Network\n(AUC = %.3f)", auc_nn)),
  cbind(rf_interp, model = sprintf("Random Forest\n(AUC = %.3f)", auc_rf))
)

# Create the plot
ggplot(plot_data, aes(x = fpr, y = tpr, color = model)) +
  geom_line(linewidth = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Average ROC Curves Across Stocks by Model Type",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    color = "Model"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5),
    panel.grid.minor = element_blank(),
    legend.text = element_text(lineheight = 0.8)
  ) +
  coord_equal() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  guides(color = guide_legend(ncol = 5))
```

***BAR CHART FOR ASSIGNMENT'S ACCURACY COMPARISON ACROSS MODELS***
```{r}
# Create vectors for each metric
barchart_mae <- c(0.1223, 0.1100, 0.1050, 0.1094, 0.1530)  # GARCH, DT, RF, GBM, NN
barchart_rmse <- c(0.1514, 0.1462, 0.1373, 0.1424, 0.2005)  # GARCH, DT, RF, GBM, NN
barchart_models <- c("GARCH", "Decision Tree", "Random Forest", "Gradient-Boosted\nDecision Tree", "Neural Network")

# Create a data frame in long format for ggplot
barchart_data <- data.frame(
  Model = rep(barchart_models, 2),
  Metric = rep(c("MAE", "RMSE"), each = length(barchart_models)),
  Value = c(barchart_mae, barchart_rmse)
)

# Create the plot
library(ggplot2)
barchart_plot <- ggplot(barchart_data, aes(x = Model, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  theme_minimal() +
  labs(
    title = "Average Error Metrics by Model",
    x = "Model",
    y = "Error Value",
    fill = "Metric"
  ) +
  scale_fill_manual(values = c("MAE" = "#69b3a2", "RMSE" = "#404080")) +
  theme(
    plot.title = element_text(hjust = 0.5),
    axis.text.x = element_text(angle = 0, hjust = 0.5),
    plot.margin = unit(c(1, 1, 1, 1), "cm"),
    legend.position = "bottom"
  )

# Display the plot
print(barchart_plot)





```


***Summary STATISTICS TABLE FOR 3-***

```{r}
# First, let's check the structure of your data
print("Data structure:")
str(jpm_maindata$RET)

# Convert RET columns to numeric if they aren't already
jpm_maindata$RET <- as.numeric(as.character(jpm_maindata$RET))
apple_maindata$RET <- as.numeric(as.character(apple_maindata$RET))
tesla_maindata$RET <- as.numeric(as.character(tesla_maindata$RET))

# Now try the statistics calculation
#Summary Statistics Table 1
library(moments)  # Load library for skewness and kurtosis

# Function to calculate summary statistics
calculate_stats <- function(data) {
  stats <- data.frame(
    Mean = mean(data, na.rm = TRUE),
    Std_Dev = sd(data, na.rm = TRUE),
    Skewness = skewness(data, na.rm = TRUE),
    Kurtosis = kurtosis(data, na.rm = TRUE),
    Min = min(data, na.rm = TRUE),
    Max = max(data, na.rm = TRUE)
  )
  return(stats)
}

# Calculate statistics for individual assets
JPM_stats <- calculate_stats(jpm_maindata$RET)
AAPL_stats <- calculate_stats(apple_maindata$RET)
TSLA_stats <- calculate_stats(tesla_maindata$RET)

# Combine into a single table
Table_1 <- rbind(
  JPM = JPM_stats,
  AAPL = AAPL_stats,
  TSLA = TSLA_stats
)

# Print the table
print(Table_1)
```

